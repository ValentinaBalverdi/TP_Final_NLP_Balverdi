{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "fvWduDsrNM9J",
        "3k-eVJQaIAd7",
        "fRvyRv58UPCW",
        "u1txpzCvUYG4",
        "EtHsaHlHOCaH",
        "IvAUMzX7DESr",
        "44przoj4DPw7",
        "oTP2kA5YDZPp",
        "i5krsWv3TuW0",
        "2m3zgXIq0w0y",
        "5Y3mSeXX1QFK",
        "pK72A3ln1IKC"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Trabajo Práctico Final 2025 - Procesamiento del Lenguaje Natural - TUIA**\n",
        "\n",
        "**Alumna**: Valentina Balverdi\n",
        "\n",
        "**Profesores:**\n",
        "* Juan Pablo Manson\n",
        "* Alan Geray\n",
        "* Constantino Ferrucci\n"
      ],
      "metadata": {
        "id": "vWVnVE5-3GBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo a usar"
      ],
      "metadata": {
        "id": "AboECeaCNIUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_gemini = \"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "2-K3lgyp8czL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalaciones"
      ],
      "metadata": {
        "id": "fvWduDsrNM9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers chromadb\n",
        "!pip install langchain-text-splitters\n",
        "!pip install chromadb\n",
        "!pip install txtai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYBIwxY9NPxN",
        "outputId": "ae308783-2301-44a8-b2a7-75786c77a6b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.3)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.47)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.3.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.3.5)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.12.3)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.3)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (9.1.2)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.39.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: txtai in /usr/local/lib/python3.12/dist-packages (9.2.0)\n",
            "Requirement already satisfied: faiss-cpu>=1.7.1.post2 in /usr/local/lib/python3.12/dist-packages (from txtai) (1.13.1)\n",
            "Requirement already satisfied: msgpack>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from txtai) (1.1.2)\n",
            "Requirement already satisfied: torch>=2.4 in /usr/local/lib/python3.12/dist-packages (from txtai) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.46.3 in /usr/local/lib/python3.12/dist-packages (from txtai) (4.57.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from txtai) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.18.4 in /usr/local/lib/python3.12/dist-packages (from txtai) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from txtai) (6.0.3)\n",
            "Requirement already satisfied: regex>=2022.8.17 in /usr/local/lib/python3.12/dist-packages (from txtai) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.5 in /usr/local/lib/python3.12/dist-packages (from txtai) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu>=1.7.1.post2->txtai) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->txtai) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->txtai) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->txtai) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->txtai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->txtai) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->txtai) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4->txtai) (3.5.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.3->txtai) (0.22.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4->txtai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4->txtai) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.34.0->txtai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.34.0->txtai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.34.0->txtai) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.34.0->txtai) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Descarga de archivos desde Drive"
      ],
      "metadata": {
        "id": "3k-eVJQaIAd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "IVZszamJSt4p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CSV a documento"
      ],
      "metadata": {
        "id": "fRvyRv58UPCW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SutJ-8TH259C",
        "outputId": "b0064257-8705-4ed5-fc9d-cdcad8d55a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/TUIA/NLP/TP_FINAL/fuentes_de_informacion\"\n",
        "\n",
        "# Carga de archivos tabulares\n",
        "path = BASE_PATH\n",
        "\n",
        "devoluciones = pd.read_csv(os.path.join(path, \"devoluciones.csv\"))\n",
        "inventario = pd.read_csv(os.path.join(path, \"inventario_sucursales.csv\"))\n",
        "productos = pd.read_csv(os.path.join(path, \"productos.csv\"))\n",
        "ventas = pd.read_csv(os.path.join(path, \"ventas_historicas.csv\"))\n",
        "vendedores = pd.read_csv(os.path.join(path, \"vendedores.csv\"))\n",
        "tickets = pd.read_csv(os.path.join(path, \"tickets_soporte.csv\"))\n",
        "\n",
        "with open(os.path.join(path, \"faqs.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    faqs = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TXT a documento"
      ],
      "metadata": {
        "id": "d-a8b6CjUVh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de textos\n",
        "resenas_dir = os.path.join(path, \"resenas_usuarios\")\n",
        "reviews_files = glob.glob(os.path.join(resenas_dir, \"*.txt\"))\n",
        "\n",
        "reviews_docs = []\n",
        "\n",
        "for fp in reviews_files:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Separar encabezado de cuerpo (una línea en blanco entre ambos)\n",
        "    partes = text.split(\"\\n\\n\", 1)\n",
        "    header = partes[0]\n",
        "    cuerpo = partes[1] if len(partes) > 1 else \"\"\n",
        "\n",
        "    # Inicializar campos\n",
        "    fecha = usuario = telefono = producto_nombre = producto_id = puntaje = provincia = None\n",
        "\n",
        "    for line in header.splitlines():\n",
        "        line = line.strip()\n",
        "        if line.startswith(\"Fecha:\"):\n",
        "            fecha = line.replace(\"Fecha:\", \"\").strip()\n",
        "        elif line.startswith(\"Usuario:\"):\n",
        "            usuario = line.replace(\"Usuario:\", \"\").strip()\n",
        "        elif line.startswith(\"Teléfono:\"):\n",
        "            telefono = line.replace(\"Teléfono:\", \"\").strip()\n",
        "        elif line.startswith(\"Producto:\"):\n",
        "            # Ej: \"Producto: Profesional Batidora de Mano (P0017)\"\n",
        "            m = re.match(r\"Producto:\\s*(.+)\\s+\\((P\\d+)\\)\", line)\n",
        "            if m:\n",
        "                producto_nombre = m.group(1).strip()\n",
        "                producto_id = m.group(2).strip()\n",
        "        elif line.startswith(\"Puntaje:\"):\n",
        "            # Ej: \"Puntaje: 5/5\"\n",
        "            puntaje = line.replace(\"Puntaje:\", \"\").strip()\n",
        "        elif line.startswith(\"Provincia:\"):\n",
        "            provincia = line.replace(\"Provincia:\", \"\").strip()\n",
        "\n",
        "    reviews_docs.append({\n",
        "        \"content\": cuerpo.strip(),              # texto que se embebe\n",
        "        \"source\": os.path.basename(fp),\n",
        "        \"type\": \"review\",\n",
        "        \"fecha\": fecha,\n",
        "        \"usuario\": usuario,\n",
        "        \"telefono\": telefono,\n",
        "        \"producto_id\": producto_id,\n",
        "        \"producto_nombre\": producto_nombre,\n",
        "        \"puntaje\": puntaje,\n",
        "        \"provincia\": provincia,\n",
        "    })\n"
      ],
      "metadata": {
        "id": "GSeLBX4QGDSO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8OWdRLbD29X",
        "outputId": "3eef1db9-43c1-4e03-c2c6-5a0806754658"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': 'Buenas! Llegó rápido y funciona de maravilla con Aire Portátil Pro. Lo compré hace una semana y es muy fácil de usar, además de moderno. Ya se lo recomendé a varios amigos. Saludos!',\n",
              " 'source': 'resena_R03514.txt',\n",
              " 'type': 'review',\n",
              " 'fecha': '2024-07-24',\n",
              " 'usuario': 'Bianca_Romero',\n",
              " 'telefono': '+54 9 367 8149-1834',\n",
              " 'producto_id': 'P0156',\n",
              " 'producto_nombre': 'Aire Portátil Pro',\n",
              " 'puntaje': '5/5',\n",
              " 'provincia': 'Neuquén'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### .md y JSON a documento"
      ],
      "metadata": {
        "id": "u1txpzCvUYG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Manuales .md\n",
        "manuales_dir = os.path.join(path, \"manuales_productos\")\n",
        "manuales_files = glob.glob(os.path.join(manuales_dir, \"*.md\"))\n",
        "\n",
        "manuales_docs = []\n",
        "\n",
        "for fp in manuales_files:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # --- Extraer información del encabezado ---\n",
        "    # Título: \"# Manual Técnico - Compacto Licuadora\"\n",
        "    titulo_match = re.search(r\"# Manual Técnico - (.+)\", text)\n",
        "    producto_nombre = titulo_match.group(1).strip() if titulo_match else None\n",
        "\n",
        "    # Modelo: \"**Modelo:** P0004 | **Marca:** ChefMaster\"\n",
        "    modelo_match = re.search(r\"\\*\\*Modelo:\\*\\*\\s*([A-Z0-9]+)\", text)\n",
        "    producto_id = modelo_match.group(1).strip() if modelo_match else None\n",
        "\n",
        "    marca_match = re.search(r\"\\*\\*Marca:\\*\\*\\s*([A-Za-z0-9]+)\", text)\n",
        "    marca = marca_match.group(1).strip() if marca_match else None\n",
        "\n",
        "    # Nombre Comercial (en Especificaciones Técnicas)\n",
        "    comercial_match = re.search(r\"\\*\\*Nombre Comercial:\\*\\*\\s*(.+)\", text)\n",
        "    nombre_comercial = comercial_match.group(1).strip() if comercial_match else producto_nombre\n",
        "\n",
        "    # Categoría\n",
        "    categoria_match = re.search(r\"\\*\\*Categoría:\\*\\*\\s*(.+)\", text)\n",
        "    categoria = categoria_match.group(1).strip() if categoria_match else None\n",
        "\n",
        "    # Guardar documento con metadata\n",
        "    manuales_docs.append({\n",
        "        \"content\": text,\n",
        "        \"source\": os.path.basename(fp),\n",
        "        \"type\": \"manual\",\n",
        "        \"producto_id\": producto_id,\n",
        "        \"producto_nombre\": nombre_comercial,\n",
        "        \"marca\": marca,\n",
        "        \"categoria_producto\": categoria\n",
        "    })\n",
        "\n",
        "# Convertir FAQs a documentos\n",
        "faq_docs = []\n",
        "\n",
        "for item in faqs:\n",
        "    pregunta = item[\"pregunta\"]\n",
        "    respuesta = item[\"respuesta\"]\n",
        "    producto = item[\"nombre_producto\"]\n",
        "    categoria = item[\"categoria\"]\n",
        "\n",
        "    faq_docs.append({\n",
        "        \"content\": (\n",
        "            f\"PRODUCTO: {producto}\\n\"\n",
        "            f\"CATEGORÍA FAQ: {categoria}\\n\"\n",
        "            f\"PREGUNTA: {pregunta}\\n\"\n",
        "            f\"RESPUESTA: {respuesta}\"\n",
        "        ),\n",
        "        \"source\": f\"faq_{item['id_faq']}.json\",\n",
        "        \"type\": \"faq\",\n",
        "        \"id_faq\": item[\"id_faq\"],\n",
        "        \"id_producto\": item[\"id_producto\"],\n",
        "        \"producto_nombre\": item[\"nombre_producto\"],\n",
        "        \"categoria_faq\": item[\"categoria\"]\n",
        "    })\n",
        "\n",
        "\n",
        "# Tickets de soporte (texto largo)\n",
        "ticket_docs = []\n",
        "\n",
        "for _, row in tickets.iterrows():\n",
        "    texto = \"\"\n",
        "\n",
        "    # incluir tipo de problema si existe\n",
        "    if isinstance(row[\"tipo_problema\"], str):\n",
        "        texto += f\"Tipo de problema: {row['tipo_problema']}\\n\"\n",
        "\n",
        "    # incluir descripción si existe\n",
        "    if isinstance(row[\"descripcion\"], str):\n",
        "        texto += f\"Descripción: {row['descripcion']}\\n\"\n",
        "\n",
        "    # Si tenemos texto válido, lo guardamos\n",
        "    if texto.strip():\n",
        "        ticket_docs.append({\n",
        "            \"content\": texto.strip(),\n",
        "            \"source\": f\"ticket_{row['id_ticket']}\",\n",
        "            \"type\": \"ticket_soporte\",\n",
        "            \"producto_id\": row[\"id_producto\"],\n",
        "            \"producto_nombre\": row[\"nombre_producto\"],\n",
        "            \"categoria\": row[\"categoria\"],\n",
        "            \"severidad\": row[\"severidad\"],\n",
        "        })\n",
        "\n",
        "# Union para la base vectorial\n",
        "text_documents = reviews_docs + manuales_docs + faq_docs + ticket_docs\n"
      ],
      "metadata": {
        "id": "142m7lSESycJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manuales_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIu1gCWMD9CD",
        "outputId": "9e04374d-7ac9-4c46-81b3-2e436b83432d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': '# Manual Técnico - Procesadora\\n\\n**Modelo:** P0013 | **Marca:** KitchenPro\\n\\n---\\n\\n## Índice\\n\\n1. [Especificaciones Técnicas](#especificaciones-técnicas)\\n2. [Componentes Principales](#componentes-principales)\\n3. [Procedimientos de Uso](#procedimientos-de-uso)\\n4. [Compatibilidad y Relaciones](#compatibilidad-y-relaciones)\\n5. [Solución de Problemas](#solución-de-problemas)\\n6. [Mantenimiento Preventivo](#mantenimiento-preventivo)\\n7. [Información de Garantía](#información-de-garantía)\\n8. [Contacto y Soporte](#contacto-y-soporte)\\n\\n---\\n\\n## Especificaciones Técnicas\\n\\n- **Modelo:** P0013\\n- **Nombre Comercial:** Procesadora\\n- **Categoría:** Cocina - Preparación\\n- **Marca:** KitchenPro\\n- **Color:** Azul\\n- **Potencia:** 1700W\\n- **Capacidad:** 2.0L\\n- **Voltaje:** 220V\\n- **Peso Neto:** 30.1 kg\\n- **Garantía:** 24 meses\\n- **Certificaciones:** CE, RoHS, ISO 9001\\n- **Clase Energética:** A+\\n- **Origen:** Importado\\n\\n## Componentes Principales\\n\\n### Motor Motor AC Universal\\n**Código:** `MOT-P0013-001`\\n\\nMotor de 1700W con Rodamientos de bolas sellados, Protección térmica integrada, Bobinado de cobre 100%\\n\\n### Conjunto de Cuchillas Titanio\\n**Código:** `CUC-P0013-002`\\n\\nCuchillas tipo Cuchilla dentada. Filo láser de precisión, Resistente a la corrosión, Desmontables para limpieza\\n\\n### Jarra/Bowl de Procesamiento\\n**Código:** `JAR-P0013-003`\\n\\nCapacidad 2.0L. Material: Vidrio borosilicato resistente a temperaturas de -20°C a 180°C. Libre de BPA. Marcas de medición en ml y oz.\\n\\n### Base con Display LED\\n**Código:** `BAS-P0013-004`\\n\\nBase de abs de alto impacto. Indicadores luminosos, Temporizador programable, Bloqueo de seguridad para niños\\n\\n### Sistema de Seguridad Integrado\\n**Código:** `SEG-P0013-005`\\n\\nBloqueo de tapa, Apagado automático, Interruptor de sobrecarga, Sistema anti-derrames\\n\\n## Procedimientos de Uso\\n\\n### PROCEDIMIENTO 1: Amasar Masa de Pan\\n\\n**Dificultad:** Medio | **Tiempo:** 12-15 minutos\\n\\n**Pasos:**\\n\\n1. Instalar el gancho amasador en el eje central del bowl\\n2. Colocar harina, sal y levadura seca en el bowl\\n3. Mezclar ingredientes secos en velocidad 1 por 15 segundos\\n4. Agregar agua tibia (no más de 40°C) gradualmente por el tubo\\n5. Continuar mezclando en velocidad 2 durante 1 minuto\\n6. Aumentar a velocidad 3-4 y amasar por 8-10 minutos\\n7. La masa debe despegarse de las paredes y formar una bola\\n8. Si la masa está pegajosa, agregar harina de a 1 cucharada\\n9. Si está muy seca, agregar agua de a 1 cucharadita\\n10. Amasar hasta obtener masa elástica y suave\\n11. Detener la procesadora y retirar el bowl\\n12. Formar la masa en bola y dejar levar según receta\\n\\n### PROCEDIMIENTO 2: Rallar Queso Duro\\n\\n**Dificultad:** Fácil | **Tiempo:** 5-7 minutos\\n\\n**Pasos:**\\n\\n1. Instalar el disco rallador grueso en el eje superior\\n2. Cortar el queso en cubos de 3x3 cm aproximadamente\\n3. Enfriar el queso en freezer por 10-15 minutos (facilita rallado)\\n4. Colocar el bowl en la base y asegurar la tapa\\n5. Encender la procesadora en velocidad media\\n6. Introducir los cubos de queso por el tubo de alimentación\\n7. Usar el empujador de alimentos, NUNCA los dedos\\n8. Presionar con firmeza pero sin forzar el motor\\n9. Procesar hasta que todo el queso esté rallado\\n10. Apagar y desconectar antes de abrir\\n11. Retirar el disco rallador con cuidado (está afilado)\\n12. Transferir el queso rallado a un recipiente\\n\\n### PROCEDIMIENTO 3: Picar Vegetales\\n\\n**Dificultad:** Fácil | **Tiempo:** 3-5 minutos\\n\\n**Pasos:**\\n\\n1. Instalar cuchilla en S multiuso en el bowl\\n2. Lavar y pelar los vegetales según sea necesario\\n3. Cortar en trozos de 2-3 cm (facilita procesamiento uniforme)\\n4. Llenar el bowl máximo hasta 3/4 de su capacidad\\n5. Cerrar la tapa y verificar que esté bien trabada\\n6. Para picado grueso: usar PULSE en intervalos de 1 segundo\\n7. Repetir 5-8 veces según textura deseada\\n8. Para picado fino: procesar en velocidad 3 por 10-15 segundos\\n9. Detener y raspar las paredes con espátula si es necesario\\n10. Continuar procesando hasta consistencia deseada\\n11. Evitar sobre-procesar (se puede convertir en puré)\\n12. Vaciar en recipiente y usar inmediatamente o refrigerar\\n\\n## Compatibilidad y Relaciones\\n\\n### Productos Compatibles\\n\\nLos siguientes productos de la misma línea son compatibles y comparten repuestos:\\n\\n- **Advanced Batidora de Pie** (`P0022`)\\n  - Comparte: Accesorios\\n- **Profesional Abridor de Latas** (`P0035`)\\n  - Comparte: Jarra\\n- **Turbo Exprimidor** (`P0131`)\\n  - Comparte: Accesorios\\n- **Olla de Cocción Lenta** (`P0087`)\\n  - Comparte: Jarra\\n- **Turbo Microondas** (`P0047`)\\n  - Comparte: Panel de control\\n\\n### Accesorios Opcionales\\n\\n- Juego de cuchillas extra\\n- Libro de recetas incluido\\n- Espátula de silicona\\n- Vaso medidor graduado\\n\\n## Solución de Problemas\\n\\n### El aparato no enciende\\n\\n**Posibles causas:**\\n\\n- Cable de alimentación desconectado o dañado\\n- Fusible quemado en el enchufe\\n- Tapa de seguridad no cerrada correctamente\\n- Interruptor de sobrecarga activado\\n\\n**Soluciones:**\\n\\n1. Verificar que el cable esté conectado firmemente\\n2. Probar en otro tomacorriente\\n3. Asegurar que todas las piezas estén correctamente ensambladas\\n4. Revisar el manual para resetear el interruptor de sobrecarga\\n5. Si el problema persiste, contactar servicio técnico\\n\\n---\\n\\n### Ruido excesivo o vibraciones\\n\\n**Posibles causas:**\\n\\n- Piezas mal ensambladas\\n- Ingredientes muy duros o congelados\\n- Sobrecarga de alimentos en el recipiente\\n- Base no está nivelada\\n- Rodamientos desgastados\\n\\n**Soluciones:**\\n\\n1. Apagar y verificar que todas las partes estén bien colocadas\\n2. Cortar alimentos en trozos más pequeños\\n3. Reducir la cantidad de ingredientes\\n4. Colocar en superficie plana y estable\\n5. Si continúa, puede requerir servicio técnico\\n\\n---\\n\\n### Pérdida de potencia o velocidad reducida\\n\\n**Posibles causas:**\\n\\n- Sobrecarga de ingredientes\\n- Ingredientes muy duros o pegajosos\\n- Motor sobrecalentado\\n- Acumulación de residuos en componentes móviles\\n- Desgaste de componentes internos\\n\\n**Soluciones:**\\n\\n1. Reducir la cantidad de alimentos\\n2. Procesar en tandas más pequeñas\\n3. Dejar enfriar el motor 15-20 minutos\\n4. Limpiar todas las piezas móviles\\n5. Si es recurrente, solicitar revisión técnica\\n\\n---\\n\\n### Fugas o derrames\\n\\n**Posibles causas:**\\n\\n- Junta de goma deteriorada o mal colocada\\n- Recipiente con rajaduras\\n- Sobrellenado del recipiente\\n- Tapa no cerrada correctamente\\n- Rosca de la cuchilla floja\\n\\n**Soluciones:**\\n\\n1. Inspeccionar y reemplazar juntas dañadas\\n2. Verificar que no haya grietas en el recipiente\\n3. No llenar más allá de la línea MAX\\n4. Asegurar cierre hermético de la tapa\\n5. Apretar la base de cuchillas (con el aparato apagado)\\n\\n---\\n\\n### Olor a quemado\\n\\n**Posibles causas:**\\n\\n- Motor sobrecargado\\n- Uso excesivo sin pausas\\n- Ingredientes pegados en el motor\\n- Cortocircuito eléctrico\\n- Componentes internos dañados\\n\\n**Soluciones:**\\n\\n1. APAGAR INMEDIATAMENTE y desconectar\\n2. Dejar enfriar completamente (30-60 minutos)\\n3. Limpiar residuos alrededor del eje motor\\n4. Si el olor persiste, NO USAR - llevar a servicio técnico\\n5. Puede ser necesario reemplazar el motor\\n\\n---\\n\\n## Mantenimiento Preventivo\\n\\n### Mantenimiento Diario\\n\\n- Limpiar todas las partes en contacto con alimentos después de cada uso\\n- Secar completamente antes de guardar\\n- Verificar que el cable no tenga daños visibles\\n- Limpiar la base con paño húmedo (NUNCA sumergir en agua)\\n\\n### Mantenimiento Semanal\\n\\n- Limpiar las juntas de goma con agua tibia y jabón\\n- Verificar que las cuchillas estén afiladas\\n- Revisar que no haya residuos en orificios de ventilación\\n- Limpiar el área del motor con pincel suave\\n\\n### Mantenimiento Mensual\\n\\n- Inspeccionar cable y enchufe por daños\\n- Verificar que tornillos estén ajustados\\n- Lubricar ejes móviles si el manual lo indica\\n- Revisar juntas de goma por grietas o desgaste\\n- Limpiar profundamente todas las piezas desmontables\\n\\n### Mantenimiento Anual\\n\\n- Revisión técnica profesional (recomendado)\\n- Reemplazar juntas de goma desgastadas\\n- Afilar o reemplazar cuchillas si es necesario\\n- Verificar funcionamiento de sistemas de seguridad\\n- Limpieza interna del motor por técnico autorizado\\n\\n## Información de Garantía\\n\\nEste producto cuenta con **24 meses de garantía** contra defectos de fabricación.\\n\\n**La garantía cubre:**\\n- Defectos de materiales\\n- Fallas de fabricación\\n- Mal funcionamiento de componentes eléctricos\\n\\n**La garantía NO cubre:**\\n- Daños por uso indebido\\n- Desgaste normal de piezas\\n- Roturas accidentales\\n- Reparaciones no autorizadas\\n\\n## Contacto y Soporte\\n\\n**KitchenPro** - Servicio de Atención al Cliente\\n\\n- **Teléfono:** 0800-XXX-XXXX\\n- **Email:** soporte@electrohogar.com.ar\\n- **Web:** www.electrohogar.com.ar\\n- **Horario:** Lunes a Viernes 9:00 - 18:00 hs\\n\\n---\\n\\n*Documento generado: P0013 - Procesadora*\\n',\n",
              " 'source': 'manual_P0013_Procesadora.md',\n",
              " 'type': 'manual',\n",
              " 'producto_id': 'P0013',\n",
              " 'producto_nombre': 'Procesadora',\n",
              " 'marca': 'KitchenPro',\n",
              " 'categoria_producto': 'Cocina - Preparación'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "faq_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPep0T_iD-oI",
        "outputId": "703f9f9b-288e-4b74-a720-640da142259d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': 'PRODUCTO: Licuadora\\nCATEGORÍA FAQ: Especificaciones\\nPREGUNTA: ¿Qué voltaje requiere?\\nRESPUESTA: El Licuadora funciona con 12V. El consumo es de 650W. Recomendamos usar un estabilizador de tensión.',\n",
              " 'source': 'faq_FAQ00001.json',\n",
              " 'type': 'faq',\n",
              " 'id_faq': 'FAQ00001',\n",
              " 'id_producto': 'P0001',\n",
              " 'producto_nombre': 'Licuadora',\n",
              " 'categoria_faq': 'Especificaciones'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ticket_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWMmzuIwEAML",
        "outputId": "e519c075-3e52-4769-d630-fce11ff53831"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'content': 'Tipo de problema: Sobrecalentamiento\\nDescripción: El producto se calienta excesivamente durante el uso',\n",
              " 'source': 'ticket_TKT000001',\n",
              " 'type': 'ticket_soporte',\n",
              " 'producto_id': 'P0251',\n",
              " 'producto_nombre': 'Pro Lavaseca',\n",
              " 'categoria': 'Eléctrico',\n",
              " 'severidad': 'Alta'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Información de los datos"
      ],
      "metadata": {
        "id": "NwGEhJ3_EGKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== FAQs ===\")\n",
        "print(\"Cantidad de FAQs:\", len(faqs))\n",
        "print(\"Ejemplo FAQ[0]:\")\n",
        "print(faqs[0])\n",
        "\n",
        "print(\"\\n=== ticket_docs ===\")\n",
        "print(\"Cantidad de ticket_docs:\", len(ticket_docs))\n",
        "print(\"Ejemplo ticket_docs[0]:\")\n",
        "print(ticket_docs[0])\n",
        "\n",
        "print(\"\\n=== DataFrames tabulares ===\")\n",
        "for nombre, df in {\n",
        "    \"devoluciones\": devoluciones,\n",
        "    \"inventario\": inventario,\n",
        "    \"productos\": productos,\n",
        "    \"ventas\": ventas,\n",
        "    \"vendedores\": vendedores,\n",
        "    \"tickets_df\": tickets,\n",
        "}.items():\n",
        "    print(f\"\\n[{nombre}] shape={df.shape}\")\n",
        "    print(\"Columnas:\", list(df.columns))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIFb10jbHnOT",
        "outputId": "428b9830-bc6c-4acc-8933-54b5e5510b50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FAQs ===\n",
            "Cantidad de FAQs: 3000\n",
            "Ejemplo FAQ[0]:\n",
            "{'id_faq': 'FAQ00001', 'id_producto': 'P0001', 'nombre_producto': 'Licuadora', 'categoria': 'Especificaciones', 'pregunta': '¿Qué voltaje requiere?', 'respuesta': 'El Licuadora funciona con 12V. El consumo es de 650W. Recomendamos usar un estabilizador de tensión.', 'fecha_publicacion': '2025-01-05', 'vistas': 4067, 'util': 22}\n",
            "\n",
            "=== ticket_docs ===\n",
            "Cantidad de ticket_docs: 2000\n",
            "Ejemplo ticket_docs[0]:\n",
            "{'content': 'Tipo de problema: Sobrecalentamiento\\nDescripción: El producto se calienta excesivamente durante el uso', 'source': 'ticket_TKT000001', 'type': 'ticket_soporte', 'producto_id': 'P0251', 'producto_nombre': 'Pro Lavaseca', 'categoria': 'Eléctrico', 'severidad': 'Alta'}\n",
            "\n",
            "=== DataFrames tabulares ===\n",
            "\n",
            "[devoluciones] shape=(800, 14)\n",
            "Columnas: ['id_devolucion', 'id_venta', 'fecha_devolucion', 'id_producto', 'nombre_producto', 'cliente_nombre', 'motivo', 'descripcion_cliente', 'estado', 'monto_venta', 'monto_reembolso', 'metodo_reembolso', 'fecha_reembolso', 'observaciones']\n",
            "\n",
            "[inventario] shape=(4100, 14)\n",
            "Columnas: ['id_inventario', 'sucursal', 'id_producto', 'nombre_producto', 'categoria', 'marca', 'stock_sucursal', 'stock_minimo', 'stock_maximo', 'precio_sucursal', 'ultima_reposicion', 'proveedor', 'pasillo', 'estado']\n",
            "\n",
            "[productos] shape=(300, 14)\n",
            "Columnas: ['id_producto', 'nombre', 'categoria', 'subcategoria', 'marca', 'precio_usd', 'stock', 'color', 'potencia_w', 'capacidad', 'voltaje', 'peso_kg', 'garantia_meses', 'descripcion']\n",
            "\n",
            "[ventas] shape=(10000, 15)\n",
            "Columnas: ['id_venta', 'fecha', 'hora', 'id_producto', 'nombre_producto', 'id_vendedor', 'nombre_vendedor', 'sucursal', 'cantidad', 'precio_unitario', 'descuento_pct', 'total', 'metodo_pago', 'cliente_nombre', 'cliente_provincia']\n",
            "\n",
            "[vendedores] shape=(100, 10)\n",
            "Columnas: ['id_vendedor', 'nombre', 'apellido', 'email', 'telefono', 'sucursal', 'fecha_ingreso', 'nivel', 'comision_pct', 'activo']\n",
            "\n",
            "[tickets_df] shape=(2000, 17)\n",
            "Columnas: ['id_ticket', 'fecha_apertura', 'id_venta', 'id_producto', 'nombre_producto', 'cliente_nombre', 'cliente_provincia', 'tipo_problema', 'descripcion', 'severidad', 'categoria', 'estado', 'vendedor_asignado', 'sucursal', 'fecha_resolucion', 'dias_resolucion', 'garantia_valida']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stats_longitudes(docs, nombre):\n",
        "    longitudes = [len(d[\"content\"]) for d in docs]\n",
        "    if not longitudes:\n",
        "        print(f\"No hay documentos en {nombre}\")\n",
        "        return None\n",
        "\n",
        "    serie = pd.Series(longitudes)\n",
        "    print(f\"=== {nombre} ===\")\n",
        "    print(\"Cantidad de documentos:\", len(longitudes))\n",
        "    print(\"Mínimo:\", int(serie.min()))\n",
        "    print(\"Máximo:\", int(serie.max()))\n",
        "    print(\"Promedio:\", int(serie.mean()))\n",
        "    print(\"Mediana:\", int(serie.median()))\n",
        "    print(\"Percentil 75:\", int(serie.quantile(0.75)))\n",
        "    print(\"Percentil 90:\", int(serie.quantile(0.90)))\n",
        "    print()\n",
        "    return serie\n",
        "\n",
        "len_reviews  = stats_longitudes(reviews_docs,  \"Reseñas\")\n",
        "len_manuales = stats_longitudes(manuales_docs, \"Manuales\")\n",
        "len_faqs     = stats_longitudes(faq_docs,      \"FAQs\")\n",
        "len_tickets  = stats_longitudes(ticket_docs,   \"Tickets soporte\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA2hgBP4lA2t",
        "outputId": "d9873109-78a6-4a60-a1fd-95ef3401ea9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reseñas ===\n",
            "Cantidad de documentos: 5015\n",
            "Mínimo: 132\n",
            "Máximo: 282\n",
            "Promedio: 200\n",
            "Mediana: 203\n",
            "Percentil 75: 223\n",
            "Percentil 90: 237\n",
            "\n",
            "=== Manuales ===\n",
            "Cantidad de documentos: 50\n",
            "Mínimo: 6450\n",
            "Máximo: 8675\n",
            "Promedio: 6623\n",
            "Mediana: 6516\n",
            "Percentil 75: 6539\n",
            "Percentil 90: 6968\n",
            "\n",
            "=== FAQs ===\n",
            "Cantidad de documentos: 3000\n",
            "Mínimo: 190\n",
            "Máximo: 365\n",
            "Promedio: 302\n",
            "Mediana: 308\n",
            "Percentil 75: 321\n",
            "Percentil 90: 332\n",
            "\n",
            "=== Tickets soporte ===\n",
            "Cantidad de documentos: 2000\n",
            "Mínimo: 74\n",
            "Máximo: 109\n",
            "Promedio: 91\n",
            "Mediana: 92\n",
            "Percentil 75: 102\n",
            "Percentil 90: 109\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 1:** RAG"
      ],
      "metadata": {
        "id": "MPAMLCbqMdnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Base de datos Vectorial**\n",
        "La base de datos vectorial se utiliza para indexar todas las fuentes de información no estructurada o semiestructurada del proyecto, incluyendo:\n",
        "\n",
        "- Manuales de producto (`.md`)\n",
        "- Reseñas de usuarios (`.txt`)\n",
        "- Preguntas frecuentes (`faqs.json`)\n",
        "- Descripciones de tickets de soporte (`tickets_soporte.csv`)\n",
        "\n",
        "Este tipo de contenido es ideal para un esquema (RAG), ya que los usuarios realizan consultas en lenguaje natural y se requiere una búsqueda semántica, no solo coincidencia literal de palabras clave."
      ],
      "metadata": {
        "id": "KnPGx85aM_PC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fragmentación del texto (Text Splitter)**\n",
        "\n",
        "Para preparar los documentos antes de generar embeddings, se utilizó el **RecursiveCharacterTextSplitter** de LangChain. Este splitter fue elegido porque:\n",
        "\n",
        "- Es robusto para textos largos como manuales.\n",
        "- Intenta mantener unidades semánticas coherentes (párrafos, oraciones) antes de cortar por caracteres.\n",
        "- Permite controlar el tamaño del fragmento (*chunk_size*) y el solapamiento (*chunk_overlap*).\n",
        "- Evita cortes innecesarios en textos cortos como reseñas, FAQs y tickets.\n",
        "\n",
        "A partir del análisis de longitudes reales de los documentos:\n",
        "\n",
        "| Fuente      | P75 longitud |\n",
        "|-------------|--------------|\n",
        "| Reseñas     | 368 chars    |\n",
        "| FAQs        | 317 chars    |\n",
        "| Tickets     | 102 chars    |\n",
        "| Manuales    | 6539 chars   |\n",
        "\n",
        "Se seleccionó:\n",
        "\n",
        "- **chunk_size = 512** → suficientemente grande para mantener intactas reseñas, FAQs y tickets,  \n",
        "  pero suficientemente pequeño para dividir manuales extensos de forma manejable.\n",
        "- **chunk_overlap = 64** (≈ 12%) → asegura continuidad semántica entre fragmentos contiguos."
      ],
      "metadata": {
        "id": "JiyDKDulipjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=64,\n",
        ")"
      ],
      "metadata": {
        "id": "bHyU3WQbioGH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_final = []\n",
        "\n",
        "# ---- Reviews ----\n",
        "for i, review_doc in enumerate(reviews_docs):\n",
        "    chunks = splitter.split_text(review_doc[\"content\"])\n",
        "    for j, chunk in enumerate(chunks):\n",
        "        chunks_final.append({\n",
        "            \"id\": f\"review_{i}_chunk_{j}\",\n",
        "            \"content\": chunk,\n",
        "            \"metadata\": {\n",
        "                \"type\": \"review\",\n",
        "                \"fuente\": review_doc[\"source\"],\n",
        "                \"chunk_index\": j,\n",
        "                \"producto_id\": review_doc.get(\"producto_id\"),\n",
        "                \"producto_nombre\": review_doc.get(\"producto_nombre\"),\n",
        "                \"puntaje\": review_doc.get(\"puntaje\"),\n",
        "                \"provincia\": review_doc.get(\"provincia\"),\n",
        "            }\n",
        "        })\n",
        "\n",
        "# ---- Manuales ----\n",
        "for i, manual_doc in enumerate(manuales_docs):\n",
        "    chunks = splitter.split_text(manual_doc[\"content\"])\n",
        "    for j, chunk in enumerate(chunks):\n",
        "        chunks_final.append({\n",
        "            \"id\": f\"manual_{i}_chunk_{j}\",\n",
        "            \"content\": chunk,\n",
        "            \"metadata\": {\n",
        "                \"type\": \"manual\",\n",
        "                \"fuente\": manual_doc[\"source\"],\n",
        "                \"chunk_index\": j,\n",
        "                \"producto_id\": manual_doc.get(\"producto_id\"),\n",
        "                \"producto_nombre\": manual_doc.get(\"producto_nombre\"),\n",
        "                \"marca\": manual_doc.get(\"marca\"),\n",
        "                \"categoria_producto\": manual_doc.get(\"categoria_producto\"),\n",
        "            }\n",
        "        })\n",
        "\n",
        "# ---- FAQs ----\n",
        "for i, faq_doc in enumerate(faq_docs):\n",
        "    chunks = splitter.split_text(faq_doc[\"content\"])\n",
        "    for j, chunk in enumerate(chunks):\n",
        "        chunks_final.append({\n",
        "            \"id\": f\"faq_{i}_chunk_{j}\",\n",
        "            \"content\": chunk,\n",
        "            \"metadata\": {\n",
        "                \"type\": \"faq\",\n",
        "                \"fuente\": faq_doc[\"source\"],\n",
        "                \"chunk_index\": j,\n",
        "                \"id_faq\": faq_doc.get(\"id_faq\"),\n",
        "                \"id_producto\": faq_doc.get(\"id_producto\"),\n",
        "                \"producto_nombre\": faq_doc.get(\"producto_nombre\"),\n",
        "                \"categoria_faq\": faq_doc.get(\"categoria_faq\"),\n",
        "            }\n",
        "        })\n",
        "\n",
        "# ---- Tickets ----\n",
        "for i, ticket_doc in enumerate(ticket_docs):\n",
        "    chunks = splitter.split_text(ticket_doc[\"content\"])\n",
        "    for j, chunk in enumerate(chunks):\n",
        "        chunks_final.append({\n",
        "            \"id\": f\"ticket_{i}_chunk_{j}\",\n",
        "            \"content\": chunk,\n",
        "            \"metadata\": {\n",
        "                \"type\": \"ticket\",\n",
        "                \"fuente\": ticket_doc[\"source\"],\n",
        "                \"chunk_index\": j,\n",
        "                \"producto_id\": ticket_doc.get(\"producto_id\"),\n",
        "                \"producto_nombre\": ticket_doc.get(\"producto_nombre\"),\n",
        "                \"categoria_ticket\": ticket_doc.get(\"categoria\"),\n",
        "                \"severidad\": ticket_doc.get(\"severidad\"),\n",
        "            }\n",
        "        })\n",
        "\n",
        "print(\"Cantidad total de chunks:\", len(chunks_final))\n",
        "chunks_final[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlLT0uxSKt9K",
        "outputId": "c77ee817-4e02-4e8c-a8d6-9d8af1b40322"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad total de chunks: 10839\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'review_0_chunk_0',\n",
              " 'content': 'Buenas! Llegó rápido y funciona de maravilla con Aire Portátil Pro. Lo compré hace una semana y es muy fácil de usar, además de moderno. Ya se lo recomendé a varios amigos. Saludos!',\n",
              " 'metadata': {'type': 'review',\n",
              "  'fuente': 'resena_R03514.txt',\n",
              "  'chunk_index': 0,\n",
              "  'producto_id': 'P0156',\n",
              "  'producto_nombre': 'Aire Portátil Pro',\n",
              "  'puntaje': '5/5',\n",
              "  'provincia': 'Neuquén'}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Modelo de Embeddings**\n",
        "\n",
        "Para transformar cada fragmento en un vector numérico se seleccionó:\n",
        "\n",
        "Modelo elegido: **sentence-transformers/paraphrase-multilingual-mpnet-base-v2**\n",
        "\n",
        "Justificación:\n",
        "\n",
        "- Es un modelo multilingüe optimizado para español.\n",
        "- Destacado en tareas de similitud semántica y sistemas RAG.\n",
        "- Produce embeddings densos de 768 dimensiones con alta calidad semántica.\n",
        "- Eficiente\n",
        "- Permite comparar consultas y documentos en el mismo espacio vectorial."
      ],
      "metadata": {
        "id": "wvxs2rKe-ixk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBVeYxDzLCig",
        "outputId": "7f709a8b-b4bc-4bd4-9373-de7fa7275ae2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Almacenamiento en ChromaDB**\n",
        "\n",
        "Se utilizó ChromaDB con almacenamiento persistente, configurado con distancia coseno"
      ],
      "metadata": {
        "id": "VY8xp-3mSyfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n"
      ],
      "metadata": {
        "id": "Fs486DDILTtw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chroma_client = chromadb.PersistentClient(path=\"chroma_vector_db\")\n",
        "\n",
        "# get_or_create obtiene la colección. Si ya existe en disco, la trae tal cual.\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"electrodomesticos_vector_store\",\n",
        "    metadata={\"hnsw:space\": \"cosine\"}\n",
        ")\n",
        "\n",
        "# VALIDACIÓN DE EXISTENCIA: Solo cargamos si está vacía\n",
        "cantidad_actual = collection.count()\n",
        "\n",
        "if cantidad_actual == 0:\n",
        "    print(\"⚡ Colección vacía. Iniciando procesamiento e ingesta de documentos...\")\n",
        "\n",
        "    # Recién acá hacemos el embedding (lo que tarda)\n",
        "    documents = [c[\"content\"] for c in chunks_final]\n",
        "    metadatas = [c[\"metadata\"] for c in chunks_final]\n",
        "    ids       = [c[\"id\"]       for c in chunks_final]\n",
        "\n",
        "    embeddings = embedding_model.encode(\n",
        "        documents,\n",
        "        batch_size=64,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "    # Carga en lotes (Batching)\n",
        "    BATCH_SIZE = 5000\n",
        "    total_docs = len(documents)\n",
        "\n",
        "    for i in range(0, total_docs, BATCH_SIZE):\n",
        "        end = min(i + BATCH_SIZE, total_docs)\n",
        "        print(f\"Insertando batch {i} a {end}...\")\n",
        "        collection.add(\n",
        "            documents=documents[i:end],\n",
        "            metadatas=metadatas[i:end],\n",
        "            ids=ids[i:end],\n",
        "            embeddings=embeddings[i:end]\n",
        "        )\n",
        "    print(\"✅ Carga finalizada.\")\n",
        "else:\n",
        "    print(f\"✅ La base vectorial ya contiene {cantidad_actual} documentos. Se omite la ingesta.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9UJMcOP5mmR",
        "outputId": "15c09f7b-5f74-47b5-cec2-75e9cfabf435"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ La base vectorial ya contiene 10839 documentos. Se omite la ingesta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Función de Búsqueda Vectorial**\n",
        "\n",
        "Esta función es el método que utiliza el sistema para recuperar fragmentos relevantes.\n",
        "Incluye:\n",
        "- Normalización de filtros ($and automático)\n",
        "- Manejo de errores\n",
        "- Ordenamiento por relevancia\n",
        "- Soporte opcional para embeddings de los documentos (útil para reranking)"
      ],
      "metadata": {
        "id": "V3dDaHSCLZwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizar_filtros(filtros: dict | None) -> dict | None:\n",
        "    \"\"\"\n",
        "    Normaliza los filtros para Chroma:\n",
        "    - Si no hay filtros (None o {}), devuelve None.\n",
        "    - Si se pasa un dict simple {\"campo\": \"valor\"}, lo deja igual.\n",
        "    - Si se arma un $and / $or, se asegura de que tenga al menos 2 condiciones.\n",
        "      Si tiene solo 1, devuelve solo esa condición.\n",
        "    \"\"\"\n",
        "    # Caso sin filtros\n",
        "    if not filtros:  # None o {}\n",
        "        return None\n",
        "\n",
        "    # Si ya viene con operadores ($and, $or, $not) lo tratamos aparte\n",
        "    if any(key.startswith(\"$\") for key in filtros):\n",
        "        # Caso $and / $or con lista vacía o de un solo elemento\n",
        "        if \"$and\" in filtros:\n",
        "            condiciones = filtros[\"$and\"]\n",
        "            if not condiciones:\n",
        "                return None\n",
        "            if len(condiciones) == 1:\n",
        "                return condiciones[0]\n",
        "            return filtros\n",
        "\n",
        "        if \"$or\" in filtros:\n",
        "            condiciones = filtros[\"$or\"]\n",
        "            if not condiciones:\n",
        "                return None\n",
        "            if len(condiciones) == 1:\n",
        "                return condiciones[0]\n",
        "            return filtros\n",
        "\n",
        "        # Otros operadores ($not, etc.)\n",
        "        return filtros\n",
        "\n",
        "    # Si es un dict simple sin operadores\n",
        "    condiciones = [{clave: valor} for clave, valor in filtros.items()]\n",
        "\n",
        "    if len(condiciones) == 0:\n",
        "        return None\n",
        "    if len(condiciones) == 1:\n",
        "        # Un solo filtro: lo devolvemos sin $and\n",
        "        return condiciones[0]\n",
        "\n",
        "    # Varios filtros simples: los combinamos con $and\n",
        "    return {\"$and\": condiciones}\n",
        "\n",
        "\n",
        "\n",
        "def buscar_vectorial(consulta, k=5, filtros=None, devolver_embeddings=False):\n",
        "    \"\"\"\n",
        "    Realiza búsqueda vectorial en ChromaDB.\n",
        "\n",
        "    Parámetros:\n",
        "        consulta (str): texto de la consulta del usuario\n",
        "        k (int): cantidad de resultados a recuperar\n",
        "        filtros (dict): condiciones para filtrar metadata\n",
        "        devolver_embeddings (bool): si se desea incluir embeddings del resultado\n",
        "                                     (útil para rerankers tipo CrossEncoder)\n",
        "\n",
        "    Devuelve:\n",
        "        lista de dicts: cada uno con id, contenido, metadata y distancia\n",
        "    \"\"\"\n",
        "    # 1. Embedding de la consulta\n",
        "    query_emb = embedding_model.encode([consulta])[0]\n",
        "\n",
        "    # 2. Normalizar filtros a formato aceptado por Chroma\n",
        "    where = normalizar_filtros(filtros)\n",
        "\n",
        "    # 3. Ejecutar consulta en Chroma\n",
        "    if where:\n",
        "        result = collection.query(\n",
        "            query_embeddings=[query_emb],\n",
        "            n_results=k,\n",
        "            where=where\n",
        "        )\n",
        "    else:\n",
        "        result = collection.query(\n",
        "            query_embeddings=[query_emb],\n",
        "            n_results=k\n",
        "        )\n",
        "\n",
        "    # 4. Si no encuentra nada\n",
        "    if len(result[\"ids\"][0]) == 0:\n",
        "        return []\n",
        "\n",
        "    # 5. Construir resultados limpios\n",
        "    resultados = []\n",
        "    for i in range(len(result[\"ids\"][0])):\n",
        "        entrada = {\n",
        "            \"id\": result[\"ids\"][0][i],\n",
        "            \"content\": result[\"documents\"][0][i],\n",
        "            \"metadata\": result[\"metadatas\"][0][i],\n",
        "            \"distance\": result[\"distances\"][0][i]\n",
        "        }\n",
        "\n",
        "        if devolver_embeddings:\n",
        "            entrada[\"embedding_doc\"] = result[\"embeddings\"][0][i]\n",
        "\n",
        "        resultados.append(entrada)\n",
        "\n",
        "    # 6. Ordenar por distancia (menor = más relevante)\n",
        "    resultados.sort(key=lambda x: x[\"distance\"])\n",
        "\n",
        "    return resultados\n"
      ],
      "metadata": {
        "id": "c17D_d3VoJ40"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prueba = buscar_vectorial(\n",
        "    \"¿Cómo usar la licuadora para hacer un smoothie?\",\n",
        "    k=3,\n",
        "    filtros={\"type\": \"manual\", \"producto_nombre\": \"Compacto Licuadora\"}\n",
        ")\n",
        "\n",
        "for r in prueba:\n",
        "    print(r[\"metadata\"])\n",
        "    print(r[\"content\"][:200], \"...\\n---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km26N5NZQDDl",
        "outputId": "171ece18-2eca-4894-9311-7ebf3b85555b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'marca': 'ChefMaster', 'fuente': 'manual_P0004_Compacto_Licuadora.md', 'chunk_index': 5, 'categoria_producto': 'Cocina - Preparación', 'producto_id': 'P0004', 'producto_nombre': 'Compacto Licuadora', 'type': 'manual'}\n",
            "## Procedimientos de Uso\n",
            "\n",
            "### PROCEDIMIENTO 1: Preparar Smoothie de Frutas\n",
            "\n",
            "**Dificultad:** Fácil | **Tiempo:** 3-5 minutos\n",
            "\n",
            "**Pasos:** ...\n",
            "---\n",
            "{'categoria_producto': 'Cocina - Preparación', 'producto_nombre': 'Compacto Licuadora', 'producto_id': 'P0004', 'fuente': 'manual_P0004_Compacto_Licuadora.md', 'chunk_index': 6, 'type': 'manual', 'marca': 'ChefMaster'}\n",
            "1. Lavar y cortar las frutas en trozos medianos (2-3 cm)\n",
            "2. Colocar los ingredientes líquidos primero (leche, yogurt, jugo)\n",
            "3. Agregar las frutas y hielo en la jarra\n",
            "4. Cerrar la tapa asegurándose del ...\n",
            "---\n",
            "{'categoria_producto': 'Cocina - Preparación', 'fuente': 'manual_P0004_Compacto_Licuadora.md', 'marca': 'ChefMaster', 'chunk_index': 7, 'type': 'manual', 'producto_id': 'P0004', 'producto_nombre': 'Compacto Licuadora'}\n",
            "9. Verificar consistencia y procesar 10-15 seg adicionales si hace falta\n",
            "10. Apagar y desconectar antes de retirar la jarra\n",
            "11. Servir inmediatamente para mejor sabor y textura ...\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_desde_vectorial_con_llm(query_usuario: str, fragmentos: list[dict], client):\n",
        "    \"\"\"\n",
        "    Toma la consulta del usuario + los fragmentos recuperados de Chroma\n",
        "    y genera una respuesta en lenguaje natural usando el LLM.\n",
        "    \"\"\"\n",
        "    if not fragmentos:\n",
        "        contexto = \"No se encontraron fragmentos relevantes.\"\n",
        "    else:\n",
        "        partes = []\n",
        "        for i, frag in enumerate(fragmentos):\n",
        "            meta = frag.get(\"metadata\", {})\n",
        "            fuente = meta.get(\"fuente\", \"desconocida\")\n",
        "            partes.append(\n",
        "                f\"[{i+1}] (fuente: {fuente})\\n{frag['content']}\"\n",
        "            )\n",
        "        contexto = \"\\n\\n\".join(partes)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Sos un asistente de soporte de una empresa de electrodomésticos.\n",
        "\n",
        "Usá EXCLUSIVAMENTE la siguiente información recuperada (fragmentos de manuales, reseñas, FAQs o tickets de soporte)\n",
        "para responder la pregunta del usuario.\n",
        "\n",
        "Si la información no alcanza para responder con seguridad, decí claramente que no hay suficiente información\n",
        "y sugerí reformular la pregunta.\n",
        "\n",
        "Pregunta del usuario:\n",
        "{query_usuario}\n",
        "\n",
        "Fragmentos recuperados:\n",
        "{contexto}\n",
        "\n",
        "Respuesta (en español, clara y directa):\n",
        "\"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=modelo_gemini,\n",
        "        contents=[prompt]\n",
        "    )\n",
        "\n",
        "    return response.text.strip()\n"
      ],
      "metadata": {
        "id": "lgxSTz40tkfA"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Base Tabular** (datos estadísticos)\n",
        "Para resolver consultas donde el usuario necesita información estructurada.\n",
        "\n",
        "Las tablas provienen de los archivos CSV del dataset:\n",
        "- productos.csv\n",
        "- ventas_historicas.csv\n",
        "- inventario_sucursales.csv\n",
        "- devoluciones.csv\n",
        "- vendedores.csv\n",
        "- tickets_soporte.csv"
      ],
      "metadata": {
        "id": "-0jk_P-ETUFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Agrupamos las tablas que vamos a usar como fuente tabular"
      ],
      "metadata": {
        "id": "EtHsaHlHOCaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tablas_tabulares = {\n",
        "    \"productos\": productos,\n",
        "    \"inventario\": inventario,\n",
        "    \"ventas\": ventas,\n",
        "    \"devoluciones\": devoluciones,\n",
        "    \"vendedores\": vendedores,\n",
        "    \"tickets\": tickets,\n",
        "}\n",
        "\n",
        "{nombre: df.shape for nombre, df in tablas_tabulares.items()}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9wzDYuwXDtf",
        "outputId": "1d656b6f-8a7a-4f48-cddf-06a73eb3f83b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'productos': (300, 14),\n",
              " 'inventario': (4100, 14),\n",
              " 'ventas': (10000, 15),\n",
              " 'devoluciones': (800, 14),\n",
              " 'vendedores': (100, 10),\n",
              " 'tickets': (2000, 17)}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de estructura de cada DataFrame para modelo"
      ],
      "metadata": {
        "id": "C1vmJfrHZrK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_estructura_para_llm(nombre_df: str, df: pd.DataFrame, max_uniques: int = 20):\n",
        "    \"\"\"\n",
        "    Genera una estructura compacta con información relevante de la tabla,\n",
        "    pensada para ser convertida a string y enviada a un LLM.\n",
        "\n",
        "    Incluye:\n",
        "    - nombre de la tabla\n",
        "    - columnas\n",
        "    - tipos de dato\n",
        "    - cantidad de filas / columnas\n",
        "    - cantidad de nulos por columna\n",
        "    - resumen por columna:\n",
        "        - numéricas: min / max\n",
        "        - categóricas: valores únicos (solo si son pocos)\n",
        "        - texto: solo se marca como texto\n",
        "    - estadísticas numéricas generales (describe), por si se quiere usar aparte\n",
        "    \"\"\"\n",
        "\n",
        "    estructura = {\n",
        "        \"nombre_df\": nombre_df,\n",
        "        \"cantidad_filas\": int(len(df)),\n",
        "        \"cantidad_columnas\": int(df.shape[1]),\n",
        "        \"columnas\": df.columns.tolist(),\n",
        "        \"tipos_de_dato\": df.dtypes.astype(str).to_dict(),\n",
        "        \"valores_nulos\": df.isnull().sum().astype(int).to_dict(),\n",
        "        \"resumen_columnas\": {},\n",
        "        \"estadisticas_numericas\": {}\n",
        "    }\n",
        "\n",
        "    # Resumen por columna\n",
        "    for col in df.columns:\n",
        "        serie = df[col]\n",
        "        col_info = {}\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(serie):\n",
        "            col_info[\"tipo_logico\"] = \"numerico\"\n",
        "            col_info[\"min\"] = float(serie.min()) if not serie.isna().all() else None\n",
        "            col_info[\"max\"] = float(serie.max()) if not serie.isna().all() else None\n",
        "\n",
        "        elif pd.api.types.is_object_dtype(serie) or pd.api.types.is_categorical_dtype(serie):\n",
        "            # Columna categórica / texto\n",
        "            n_unique = serie.nunique(dropna=True)\n",
        "            col_info[\"tipo_logico\"] = \"categorico\" if n_unique <= max_uniques else \"texto_largo\"\n",
        "            col_info[\"num_valores_unicos\"] = int(n_unique)\n",
        "\n",
        "            if n_unique <= max_uniques:\n",
        "                col_info[\"valores_unicos\"] = serie.dropna().unique().tolist()\n",
        "\n",
        "        else:\n",
        "            col_info[\"tipo_logico\"] = \"otro\"\n",
        "\n",
        "        estructura[\"resumen_columnas\"][col] = col_info\n",
        "\n",
        "    # Estadísticas numéricas (describe)\n",
        "    if not df.select_dtypes(include=\"number\").empty:\n",
        "        estructura[\"estadisticas_numericas\"] = (\n",
        "            df.select_dtypes(include=\"number\").describe().to_dict()\n",
        "        )\n",
        "\n",
        "    return estructura\n"
      ],
      "metadata": {
        "id": "UQgFPKwbXJl0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertir la estructura a texto legible para el LLM"
      ],
      "metadata": {
        "id": "2HScFhBPOQgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estructura_a_string(estructura: dict) -> str:\n",
        "    \"\"\"\n",
        "    Convierte la estructura generada por generar_estructura_para_llm\n",
        "    a un string legible para usar en el prompt del LLM.\n",
        "    \"\"\"\n",
        "    lineas = []\n",
        "    lineas.append(f\"Nombre de la tabla: {estructura['nombre_df']}\")\n",
        "    lineas.append(f\"Filas: {estructura['cantidad_filas']}, Columnas: {estructura['cantidad_columnas']}\")\n",
        "    lineas.append(\"Columnas:\")\n",
        "\n",
        "    for col, info in estructura[\"resumen_columnas\"].items():\n",
        "        tipo = info.get(\"tipo_logico\", \"desconocido\")\n",
        "\n",
        "        if tipo == \"numerico\":\n",
        "            lineas.append(\n",
        "                f\"- {col}: numérico, rango aproximado [{info.get('min')}, {info.get('max')}]\"\n",
        "            )\n",
        "        elif tipo == \"categorico\":\n",
        "            vals = \", \".join(map(str, info.get(\"valores_unicos\", [])))\n",
        "            lineas.append(\n",
        "                f\"- {col}: categórico, valores posibles: {vals}\"\n",
        "            )\n",
        "        elif tipo == \"texto_largo\":\n",
        "            lineas.append(\n",
        "                f\"- {col}: texto libre / muchas categorías (≈{info.get('num_valores_unicos')} valores distintos)\"\n",
        "            )\n",
        "        else:\n",
        "            lineas.append(f\"- {col}: tipo {tipo}\")\n",
        "\n",
        "    return \"\\n\".join(lineas)\n"
      ],
      "metadata": {
        "id": "M8Wzn-agj1gv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esquemas que se le pasan al LLM"
      ],
      "metadata": {
        "id": "Dxg6kmSwOXVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 1) Estructuras en diccionarios\n",
        "esquemas_tabulares = {\n",
        "    nombre: generar_estructura_para_llm(nombre, df)\n",
        "    for nombre, df in tablas_tabulares.items()\n",
        "}\n",
        "\n",
        "# 2) Versión legible como texto para meter en el prompt del LLM\n",
        "descripciones_tablas = {\n",
        "    nombre: estructura_a_string(estructura)\n",
        "    for nombre, estructura in esquemas_tabulares.items()\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7GDAId_oj4wQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de prompt y conexión con gemini"
      ],
      "metadata": {
        "id": "y31HUAe4Zveu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "TFCAzHqt1ufb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializamos el cliente Gemini\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "PyttALLQ9KRr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def armar_prompt_tabular(query_usuario: str) -> str:\n",
        "    \"\"\"\n",
        "    Construye el prompt que se le va a enviar al LLM para que\n",
        "    genere código de pandas que responda a la consulta del usuario.\n",
        "    \"\"\"\n",
        "\n",
        "    partes = []\n",
        "\n",
        "    partes.append(\n",
        "        \"Sos un asistente experto en análisis de datos con pandas.\\n\"\n",
        "        \"Trabajás con datos de una empresa de electrodomésticos.\\n\\n\"\n",
        "        \"Tu tarea es: dado una pregunta del usuario, responder EXCLUSIVAMENTE con código Python\\n\"\n",
        "        \"que use DataFrames de pandas ya cargados para obtener la respuesta.\\n\\n\"\n",
        "        \"IMPORTANTE:\\n\"\n",
        "        \"- NO crees DataFrames nuevos desde cero con datos escritos a mano.\\n\"\n",
        "        \"- NO uses read_csv ni lecturas de archivos.\\n\"\n",
        "        \"- NO imprimas nada, NO uses print.\\n\"\n",
        "        \"- NO expliques el código, solo devolvé la expresión de pandas.\\n\"\n",
        "        \"- La expresión debe devolver directamente el resultado (DataFrame o valor escalar).\\n\"\n",
        "        \"- Los DataFrames disponibles son:\\n\"\n",
        "        \"  * productos\\n\"\n",
        "        \"  * inventario\\n\"\n",
        "        \"  * ventas\\n\"\n",
        "        \"  * devoluciones\\n\"\n",
        "        \"  * vendedores\\n\"\n",
        "        \"  * tickets\\n\"\n",
        "    )\n",
        "\n",
        "    partes.append(\"\\nA continuación te doy un resumen de las tablas disponibles:\\n\")\n",
        "\n",
        "    for nombre, desc in descripciones_tablas.items():\n",
        "        partes.append(f\"\\n### Tabla: {nombre}\\n{desc}\\n\")\n",
        "\n",
        "    partes.append(\"\\nPregunta del usuario:\\n\")\n",
        "    partes.append(query_usuario)\n",
        "    partes.append(\n",
        "        \"\\n\\nAhora respondé ÚNICAMENTE con código Python válido que utilice pandas \"\n",
        "        \"y estos DataFrames para obtener la respuesta. No agregues explicaciones ni comentarios.\"\n",
        "    )\n",
        "\n",
        "    return \"\\n\".join(partes)\n"
      ],
      "metadata": {
        "id": "2M00kknai38R"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ejecutar_consulta_tabular(query_usuario: str):\n",
        "    \"\"\"\n",
        "    Usa el LLM (Gemini) para generar código de pandas a partir de la pregunta\n",
        "    del usuario y luego ejecuta ese código sobre los DataFrames ya cargados.\n",
        "\n",
        "    Devuelve:\n",
        "        - resultado: lo que devuelve la expresión de pandas (DataFrame, serie, escalar, etc.)\n",
        "        - codigo_generado: el string de código que generó el modelo\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Armar el prompt a partir de tu función\n",
        "    prompt = armar_prompt_tabular(query_usuario)\n",
        "\n",
        "    # 2) Llamar al modelo\n",
        "    response = client.models.generate_content(\n",
        "        model=modelo_gemini,\n",
        "        contents=[prompt]\n",
        "    )\n",
        "\n",
        "    # 3) Extraer el texto y limpiar bloque de código\n",
        "    codigo_generado = response.text.strip()\n",
        "    codigo_generado = (\n",
        "        codigo_generado\n",
        "        .replace(\"```python\", \"\")\n",
        "        .replace(\"```py\", \"\")\n",
        "        .replace(\"```\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    print(\"🔧 Código generado por el modelo:\\n\")\n",
        "    print(codigo_generado)\n",
        "    print(\"\\n---\\n\")\n",
        "\n",
        "    # 4) Ejecutar el código. Necesita que los DataFrames estén en el scope global.\n",
        "    try:\n",
        "        resultado = eval(codigo_generado, globals())\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Error al ejecutar el código generado:\")\n",
        "        print(e)\n",
        "        resultado = None\n",
        "\n",
        "    return resultado, codigo_generado\n"
      ],
      "metadata": {
        "id": "oh_ELOiIZfpy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_desde_dataframe_con_llm(query_usuario, resultado, client):\n",
        "    \"\"\"\n",
        "    Toma la pregunta del usuario + resultado pandas (DataFrame, Serie o escalar)\n",
        "    y genera una respuesta clara y natural usando el LLM.\n",
        "    \"\"\"\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    if isinstance(resultado, pd.DataFrame):\n",
        "        if resultado.empty:\n",
        "            tabla = \"Tabla vacía (sin resultados).\"\n",
        "        else:\n",
        "            tabla = resultado.to_string(index=False)\n",
        "\n",
        "    elif isinstance(resultado, pd.Series):\n",
        "        tabla = resultado.to_string()\n",
        "\n",
        "    else:\n",
        "        tabla = str(resultado)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Sos un asistente experto en análisis de datos tabulares.\n",
        "\n",
        "Te doy:\n",
        "- La pregunta original del usuario.\n",
        "- El resultado ya filtrado desde las tablas de electrodomésticos.\n",
        "\n",
        "Tu tarea:\n",
        "- Responder en español, claro y directo.\n",
        "- Explicar qué significa el resultado.\n",
        "- NO mencionar pandas, ni código, ni cómo se obtuvo.\n",
        "- Si está vacío, sugerir reformular la consulta.\n",
        "\n",
        "Pregunta del usuario:\n",
        "{query_usuario}\n",
        "\n",
        "Resultado:\n",
        "{tabla}\n",
        "\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=modelo_gemini,\n",
        "        contents=[prompt]\n",
        "    )\n",
        "\n",
        "    return response.text.strip()"
      ],
      "metadata": {
        "id": "Gh2X93bNOwl1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_tabular(query_usuario: str):\n",
        "    \"\"\"\n",
        "    Pipeline completo para la base TABULAR:\n",
        "    - Genera código de pandas con el LLM\n",
        "    - Ejecuta la expresión sobre los DataFrames\n",
        "    - Genera una respuesta explicada en lenguaje natural\n",
        "    \"\"\"\n",
        "\n",
        "    resultado, codigo = ejecutar_consulta_tabular(query_usuario)\n",
        "\n",
        "    if resultado is None:\n",
        "        return {\n",
        "            \"fuente\": \"tabular\",\n",
        "            \"codigo_generado\": codigo,\n",
        "            \"resultado_bruto\": None,\n",
        "            \"respuesta\": (\n",
        "                \"No pude ejecutar la consulta tabular. \"\n",
        "                \"Probá reformular la pregunta o revisar los campos mencionados.\"\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    respuesta_nl = responder_desde_dataframe_con_llm(query_usuario, resultado, client)\n",
        "\n",
        "    return {\n",
        "        \"fuente\": \"tabular\",\n",
        "        \"codigo_generado\": codigo,\n",
        "        \"resultado_bruto\": resultado,\n",
        "        \"respuesta\": respuesta_nl,\n",
        "    }"
      ],
      "metadata": {
        "id": "PRtIMrYIO3Xj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultado, codigo = ejecutar_consulta_tabular(\"Mostrame el top 10 de productos más vendidos por cantidad en la tabla de ventas.\")\n",
        "print(\"Resultado:\\n\", resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL0BQyd9Pb56",
        "outputId": "d7e3e6b0-4af1-4888-c969-c215b316171d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Código generado por el modelo:\n",
            "\n",
            "ventas.groupby('nombre_producto')['cantidad'].sum().nlargest(10)\n",
            "\n",
            "---\n",
            "\n",
            "Resultado:\n",
            " nombre_producto\n",
            "Licuadora              408\n",
            "Centro de Planchado    390\n",
            "Waflera                365\n",
            "Aire Split             300\n",
            "Frigobar               265\n",
            "Freidora de Aire       248\n",
            "Neblinizador           245\n",
            "Humidificador          234\n",
            "Planchita de Pelo      231\n",
            "Yogurtera              228\n",
            "Name: cantidad, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Base de datos de grafos**"
      ],
      "metadata": {
        "id": "gy130-icDDku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracción de relaciones de compatibilidad desde los manuales"
      ],
      "metadata": {
        "id": "IvAUMzX7DESr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "compat_rows = []\n",
        "\n",
        "for manual in manuales_docs:\n",
        "    texto = manual[\"content\"]\n",
        "    producto_origen = manual.get(\"producto_id\")\n",
        "    nombre_origen = manual.get(\"producto_nombre\")\n",
        "\n",
        "    # Buscar sección \"### Productos Compatibles ... (hasta la siguiente sección o el final)\"\n",
        "    patron_seccion = r\"### Productos Compatibles(.*?)(## |$)\"\n",
        "    match = re.search(patron_seccion, texto, re.DOTALL)\n",
        "\n",
        "    if not match:\n",
        "        continue  # El manual no tiene sección de compatibilidad\n",
        "\n",
        "    seccion = match.group(1)\n",
        "\n",
        "    # Buscar líneas de productos compatibles:\n",
        "    # - **Nombre Producto** (`P0001`)\n",
        "    patron_producto = r\"- \\*\\*(.*?)\\*\\* .*?\\(`(P\\d+)`\\)\"\n",
        "    productos_encontrados = re.findall(patron_producto, seccion)\n",
        "\n",
        "    # Buscar líneas con \"Comparte: X\"\n",
        "    patron_comparte = r\"Comparte:\\s*([^\\n\\r]+)\"\n",
        "    comparte_list = re.findall(patron_comparte, seccion)\n",
        "\n",
        "    # Emparejar producto destino con el texto de \"comparte\"\n",
        "    for idx, (nombre_destino, id_destino) in enumerate(productos_encontrados):\n",
        "        comparte = comparte_list[idx] if idx < len(comparte_list) else None\n",
        "\n",
        "        compat_rows.append({\n",
        "            \"id_origen\": producto_origen,\n",
        "            \"nombre_origen\": nombre_origen,\n",
        "            \"id_destino\": id_destino,\n",
        "            \"nombre_destino\": nombre_destino,\n",
        "            \"comparte\": comparte\n",
        "        })\n",
        "\n",
        "compat_df = pd.DataFrame(compat_rows)\n",
        "\n",
        "print(compat_df.shape)\n",
        "compat_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "gXeXjpHcDKtn",
        "outputId": "575da079-24aa-42d6-984a-c9b36821469b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  id_origen nombre_origen id_destino                nombre_destino  \\\n",
              "0     P0013   Procesadora      P0022      Advanced Batidora de Pie   \n",
              "1     P0013   Procesadora      P0035  Profesional Abridor de Latas   \n",
              "2     P0013   Procesadora      P0131              Turbo Exprimidor   \n",
              "3     P0013   Procesadora      P0087         Olla de Cocción Lenta   \n",
              "4     P0013   Procesadora      P0047              Turbo Microondas   \n",
              "\n",
              "           comparte  \n",
              "0        Accesorios  \n",
              "1             Jarra  \n",
              "2        Accesorios  \n",
              "3             Jarra  \n",
              "4  Panel de control  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc1d16f6-0d1c-4b0f-993b-4ff97ea577f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_origen</th>\n",
              "      <th>nombre_origen</th>\n",
              "      <th>id_destino</th>\n",
              "      <th>nombre_destino</th>\n",
              "      <th>comparte</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P0013</td>\n",
              "      <td>Procesadora</td>\n",
              "      <td>P0022</td>\n",
              "      <td>Advanced Batidora de Pie</td>\n",
              "      <td>Accesorios</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P0013</td>\n",
              "      <td>Procesadora</td>\n",
              "      <td>P0035</td>\n",
              "      <td>Profesional Abridor de Latas</td>\n",
              "      <td>Jarra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P0013</td>\n",
              "      <td>Procesadora</td>\n",
              "      <td>P0131</td>\n",
              "      <td>Turbo Exprimidor</td>\n",
              "      <td>Accesorios</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P0013</td>\n",
              "      <td>Procesadora</td>\n",
              "      <td>P0087</td>\n",
              "      <td>Olla de Cocción Lenta</td>\n",
              "      <td>Jarra</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P0013</td>\n",
              "      <td>Procesadora</td>\n",
              "      <td>P0047</td>\n",
              "      <td>Turbo Microondas</td>\n",
              "      <td>Panel de control</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc1d16f6-0d1c-4b0f-993b-4ff97ea577f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc1d16f6-0d1c-4b0f-993b-4ff97ea577f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc1d16f6-0d1c-4b0f-993b-4ff97ea577f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f264cf65-f3e8-4224-a519-2da04b420d06\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f264cf65-f3e8-4224-a519-2da04b420d06')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f264cf65-f3e8-4224-a519-2da04b420d06 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "compat_df",
              "summary": "{\n  \"name\": \"compat_df\",\n  \"rows\": 250,\n  \"fields\": [\n    {\n      \"column\": \"id_origen\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"P0120\",\n          \"P0186\",\n          \"P0296\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nombre_origen\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"Freezer\",\n          \"Advanced Deshidratador\",\n          \"Vinoteca Pro\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id_destino\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 171,\n        \"samples\": [\n          \"P0099\",\n          \"P0039\",\n          \"P0119\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nombre_destino\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 150,\n        \"samples\": [\n          \"Max Conservadora\",\n          \"Humidificador\",\n          \"Mixer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comparte\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Jarra\",\n          \"Cuchillas\",\n          \"Panel de control\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construcción de la tabla de productos para el grafo"
      ],
      "metadata": {
        "id": "44przoj4DPw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Productos que participan en alguna relación de compatibilidad\n",
        "ids_origen = compat_df[\"id_origen\"]\n",
        "ids_destino = compat_df[\"id_destino\"]\n",
        "\n",
        "ids_todos = pd.Series(\n",
        "    pd.concat([ids_origen, ids_destino]).unique(),\n",
        "    name=\"id_producto\"\n",
        ")\n",
        "\n",
        "# Nos quedamos con columnas relevantes de la tabla productos\n",
        "productos_grafo_df = (\n",
        "    ids_todos.to_frame()\n",
        "    .merge(\n",
        "        productos[[\"id_producto\", \"nombre\", \"categoria\", \"marca\"]],\n",
        "        on=\"id_producto\",\n",
        "        how=\"left\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Cantidad de productos en el grafo:\", len(productos_grafo_df))\n",
        "productos_grafo_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "OgsD9wkODSYK",
        "outputId": "1be5cf14-560e-429d-9ce6-6780cee955a2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de productos en el grafo: 192\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  id_producto              nombre      categoria       marca\n",
              "0       P0013         Procesadora         Cocina  KitchenPro\n",
              "1       P0016      Super Picadora         Cocina   CookElite\n",
              "2       P0082  Olla Arrocera 3000         Cocina    HomeChef\n",
              "3       P0149          Aire Split  Climatización    EcoClima\n",
              "4       P0050    Freidora de Aire         Cocina  KitchenPro"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64afcde0-3311-4c63-bee1-415b8c679e0e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_producto</th>\n",
              "      <th>nombre</th>\n",
              "      <th>categoria</th>\n",
              "      <th>marca</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P0013</td>\n",
              "      <td>Procesadora</td>\n",
              "      <td>Cocina</td>\n",
              "      <td>KitchenPro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P0016</td>\n",
              "      <td>Super Picadora</td>\n",
              "      <td>Cocina</td>\n",
              "      <td>CookElite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P0082</td>\n",
              "      <td>Olla Arrocera 3000</td>\n",
              "      <td>Cocina</td>\n",
              "      <td>HomeChef</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P0149</td>\n",
              "      <td>Aire Split</td>\n",
              "      <td>Climatización</td>\n",
              "      <td>EcoClima</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P0050</td>\n",
              "      <td>Freidora de Aire</td>\n",
              "      <td>Cocina</td>\n",
              "      <td>KitchenPro</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64afcde0-3311-4c63-bee1-415b8c679e0e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-64afcde0-3311-4c63-bee1-415b8c679e0e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-64afcde0-3311-4c63-bee1-415b8c679e0e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a000e497-0692-4a5f-adc7-250a23f935fd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a000e497-0692-4a5f-adc7-250a23f935fd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a000e497-0692-4a5f-adc7-250a23f935fd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "productos_grafo_df",
              "summary": "{\n  \"name\": \"productos_grafo_df\",\n  \"rows\": 192,\n  \"fields\": [\n    {\n      \"column\": \"id_producto\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 192,\n        \"samples\": [\n          \"P0036\",\n          \"P0293\",\n          \"P0044\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nombre\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 164,\n        \"samples\": [\n          \"Premium Heladera\",\n          \"Parrilla El\\u00e9ctrica\",\n          \"Compacto Rallador El\\u00e9ctrico 2024\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categoria\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Cocina\",\n          \"Climatizaci\\u00f3n\",\n          \"Lavado\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"marca\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"PureAir\",\n          \"LaundryTech\",\n          \"KitchenPro\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conexión a Neo4j y creación del grafo de productos"
      ],
      "metadata": {
        "id": "oTP2kA5YDZPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py2neo\n"
      ],
      "metadata": {
        "id": "lLwPMkHODZvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e6b563-5f25-4bf2-b429-0be8c9310185"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: py2neo in /usr/local/lib/python3.12/dist-packages (2021.2.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from py2neo) (2025.11.12)\n",
            "Requirement already satisfied: interchange~=2021.0.4 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2021.0.4)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.12/dist-packages (from py2neo) (1.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from py2neo) (25.0)\n",
            "Requirement already satisfied: pansi>=2020.7.3 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2024.11.0)\n",
            "Requirement already satisfied: pygments>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2.19.2)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from py2neo) (1.17.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from py2neo) (2.3.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from interchange~=2021.0.4->py2neo) (2025.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pansi>=2020.7.3->py2neo) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from py2neo import Graph, Node, Relationship\n",
        "\n",
        "NEO4J_URI = \"neo4j+s://1830c5fb.databases.neo4j.io\"\n",
        "NEO4J_USER = \"neo4j\"\n",
        "NEO4J_PASSWORD = userdata.get(\"NEO4_KEY\")\n",
        "\n",
        "graph = Graph(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "# Test rápido de conexión\n",
        "graph.run(\"RETURN 1 AS test\").data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZAGCGylDbpx",
        "outputId": "4717d55c-33d5-40f4-9e8d-9ea7a3a521f6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'test': 1}]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de nodos :Producto en Neo4j"
      ],
      "metadata": {
        "id": "c2RZGH0sDsUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_productos_py2neo(graph, df):\n",
        "    tx = graph.begin()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        nodo = Node(\n",
        "            \"Producto\",\n",
        "            id=row[\"id_producto\"],\n",
        "            nombre=row[\"nombre\"],\n",
        "            categoria=row[\"categoria\"],\n",
        "            marca=row[\"marca\"]\n",
        "        )\n",
        "        tx.merge(nodo, \"Producto\", \"id\")  # upsert por id\n",
        "\n",
        "    tx.commit()\n",
        "    print(f\"Se cargaron/actualizaron {len(df)} nodos :Producto.\")\n",
        "\n",
        "cargar_productos_py2neo(graph, productos_grafo_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TZ720v2Dsyj",
        "outputId": "be161967-65b3-47c5-e053-d796f369a478"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se cargaron/actualizaron 192 nodos :Producto.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1693625308.py:14: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
            "  tx.commit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de relaciones :COMPATIBLE_CON en Neo4j"
      ],
      "metadata": {
        "id": "L9FJTqiWDyIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_relaciones_py2neo(graph, df):\n",
        "    tx = graph.begin()\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        origen = graph.nodes.match(\"Producto\", id=row[\"id_origen\"]).first()\n",
        "        destino = graph.nodes.match(\"Producto\", id=row[\"id_destino\"]).first()\n",
        "\n",
        "        if origen and destino:\n",
        "            rel = Relationship(origen, \"COMPATIBLE_CON\", destino)\n",
        "            rel[\"comparte\"] = row[\"comparte\"]\n",
        "            tx.merge(rel)\n",
        "\n",
        "    tx.commit()\n",
        "    print(f\"Se cargaron/actualizaron {len(df)} relaciones :COMPATIBLE_CON.\")\n",
        "\n",
        "cargar_relaciones_py2neo(graph, compat_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShvXpJmmD0Ab",
        "outputId": "dd19c6cb-7fd5-4ffa-e2cc-cb5a04bdbcbb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se cargaron/actualizaron 250 relaciones :COMPATIBLE_CON.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4095784289.py:13: DeprecationWarning: The transaction.commit() method is deprecated, use graph.commit(transaction) instead\n",
            "  tx.commit()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper para ejecutar consultas Cypher desde Python"
      ],
      "metadata": {
        "id": "7wDA7h8AD3UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ejecutar_cypher(query: str, params: dict | None = None):\n",
        "    params = params or {}\n",
        "    return graph.run(query, params).data()"
      ],
      "metadata": {
        "id": "fF1NoYstD2or"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt para que el LLM genere queries Cypher sobre el grafo"
      ],
      "metadata": {
        "id": "zxHNqL0oEDAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def armar_prompt_grafo(query_usuario: str) -> str:\n",
        "    \"\"\"\n",
        "    Construye el prompt para que el LLM genere una query Cypher\n",
        "    sobre el grafo de productos y compatibilidades.\n",
        "    \"\"\"\n",
        "\n",
        "    return f\"\"\"\n",
        "Sos un asistente experto en Neo4j y lenguaje Cypher.\n",
        "\n",
        "TRABAJÁS CON ESTE GRAFO:\n",
        "\n",
        "Nodos:\n",
        "  (:Producto)\n",
        "    - id        (por ejemplo: \"P0013\")\n",
        "    - nombre    (por ejemplo: \"Procesadora\")\n",
        "    - categoria\n",
        "    - marca\n",
        "\n",
        "Relaciones:\n",
        "  (:Producto)-[:COMPATIBLE_CON {{comparte: <string>}}]->(:Producto)\n",
        "\n",
        "REGLAS ESTRICTAS PARA GENERAR CYPHER:\n",
        "\n",
        "- SIEMPRE usá un alias para la relación:\n",
        "      (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto)\n",
        "\n",
        "- NUNCA pongas propiedades de la relación dentro del MATCH.\n",
        "   NO hacer cosas como:\n",
        "      -[:COMPATIBLE_CON {{comparte: algo}}]->\n",
        "\n",
        "- Las propiedades de la relación se leen SOLO en el RETURN:\n",
        "      r.comparte AS componente_compartido\n",
        "\n",
        "- La query debe SIEMPRE devolver EXACTAMENTE estas columnas, en este orden:\n",
        "      c.id        AS id_producto,\n",
        "      c.nombre    AS nombre,\n",
        "      c.categoria AS categoria,\n",
        "      c.marca     AS marca,\n",
        "      r.comparte  AS componente_compartido\n",
        "\n",
        "- NO uses \"null AS componente_compartido\".\n",
        "- NO agregues texto adicional, explicaciones ni comentarios.\n",
        "- NO uses bloques ``` ni markdown.\n",
        "- La consulta debe estar compuesta por UN SOLO statement Cypher.\n",
        "\n",
        "El usuario puede preguntar en lenguaje natural por compatibilidades\n",
        "entre productos (por nombre o por id). Usá esa información para\n",
        "construir el WHERE o el patrón de MATCH correspondiente.\n",
        "\n",
        "Pregunta del usuario:\n",
        "{query_usuario}\n",
        "\n",
        "Devolvé ÚNICAMENTE la query Cypher válida que cumple todas las reglas anteriores.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gJXanqhUD8kf"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversión de lenguaje natural a Cypher y ejecución en Neo4j"
      ],
      "metadata": {
        "id": "FKfvc1w8EJ1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def consulta_grafo_con_llm(query_usuario: str):\n",
        "    \"\"\"\n",
        "    Usa el LLM para convertir una pregunta en lenguaje natural\n",
        "    a una query Cypher, ejecuta esa query en Neo4j\n",
        "    y devuelve (resultados, cypher_generado).\n",
        "    \"\"\"\n",
        "    prompt = armar_prompt_grafo(query_usuario)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=modelo_gemini,\n",
        "        contents=[prompt]\n",
        "    )\n",
        "\n",
        "    cypher_generado = (\n",
        "        response.text.replace(\"```cypher\", \"\")\n",
        "        .replace(\"```\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "\n",
        "    print(\"🔧 Cypher generado por el modelo:\\n\")\n",
        "    print(cypher_generado)\n",
        "    print(\"\\n---\\n\")\n",
        "\n",
        "    try:\n",
        "        resultados = ejecutar_cypher(cypher_generado)\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Error al ejecutar la query Cypher:\")\n",
        "        print(e)\n",
        "        resultados = []\n",
        "\n",
        "    return resultados, cypher_generado\n"
      ],
      "metadata": {
        "id": "_VhVADYvEGPi"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación de respuesta en lenguaje natural desde el resultado del grafo"
      ],
      "metadata": {
        "id": "oLT8bdjvEMkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_desde_grafo_con_llm(query_usuario: str, resultados: list[dict], client):\n",
        "    \"\"\"\n",
        "    Toma la pregunta original + los resultados (lista de diccionarios)\n",
        "    y le pide al LLM que genere una respuesta explicada.\n",
        "    \"\"\"\n",
        "    if not resultados:\n",
        "        tabla = \"Sin resultados (la consulta al grafo no devolvió filas).\"\n",
        "    else:\n",
        "        df_res = pd.DataFrame(resultados)\n",
        "        tabla = df_res.to_string(index=False)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Sos un asistente que trabaja con un grafo de productos de electrodomésticos.\n",
        "\n",
        "Te doy la pregunta original del usuario y el resultado de una consulta a Neo4j.\n",
        "El resultado ya contiene los productos compatibles (si los hay).\n",
        "\n",
        "Tu tarea:\n",
        "- Responder en español, de forma clara y directa.\n",
        "- Explicar qué productos son compatibles, si comparten componentes, etc.\n",
        "- Si no hay resultados, explicá que no se encontró compatibilidad para ese caso.\n",
        "- NO expliques la query Cypher ni los detalles internos del grafo.\n",
        "\n",
        "Pregunta del usuario:\n",
        "{query_usuario}\n",
        "\n",
        "Resultado de la consulta al grafo:\n",
        "{tabla}\n",
        "\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=modelo_gemini,\n",
        "        contents=[prompt]\n",
        "    )\n",
        "\n",
        "    return response.text.strip()"
      ],
      "metadata": {
        "id": "PKlwJ3BrENLO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline completo de la fuente \"grafo\""
      ],
      "metadata": {
        "id": "l1GysR1wESHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline_grafo(query_usuario: str):\n",
        "    resultados, cypher = consulta_grafo_con_llm(query_usuario)\n",
        "    respuesta_nl = responder_desde_grafo_con_llm(query_usuario, resultados, client)\n",
        "    return {\n",
        "        \"fuente\": \"grafo\",\n",
        "        \"cypher_generado\": cypher,\n",
        "        \"resultado_bruto\": resultados,\n",
        "        \"respuesta\": respuesta_nl,\n",
        "    }\n",
        "\n",
        "#Ejemplo de prueba puntual:\n",
        "# resp_g = pipeline_grafo(\"¿Qué productos son compatibles con la Procesadora?\")\n",
        "# print(resp_g[\"respuesta\"])"
      ],
      "metadata": {
        "id": "PiTqSI51ER0f"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Clasificador de intención avanzado**\n",
        "\n",
        "En esta sección se desarrolla un **clasificador de intención** que decide a qué fuente de datos\n",
        "debe dirigirse el sistema según la pregunta del usuario:\n",
        "\n",
        "- **Vectorial**: consultas típicas de manuales, FAQs, reseñas y problemas de uso de productos.\n",
        "- **Tabular**: consultas sobre datos estructurados (ventas, stock, devoluciones, totales, rankings, etc.).\n",
        "- **Grafo**: consultas sobre **compatibilidad** entre productos y repuestos.\n",
        "\n",
        "Para cumplir con el enunciado se implementan dos clasificadores:\n",
        "\n",
        "1. Un **clasificador entrenado propio**, basado en TF-IDF + Logistic Regression, utilizando un\n",
        "   conjunto de preguntas sintéticas representativas de cada tipo de fuente.\n",
        "2. Un **clasificador basado en LLM** (Gemini) con *few-shot prompting*, donde se le explican\n",
        "   las clases al modelo y se le dan ejemplos de preguntas ya etiquetadas.\n",
        "\n",
        "Finalmente, se comparan ambos clasificadores utilizando métricas de clasificación y se justifica\n",
        "qué enfoque resulta más adecuado para el sistema."
      ],
      "metadata": {
        "id": "UFLyqIqBEZa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset de ejemplos sintéticos"
      ],
      "metadata": {
        "id": "i5krsWv3TuW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "MODELO_GEMINI_CLF = modelo_gemini"
      ],
      "metadata": {
        "id": "yWAZXFaHWeq3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cada tupla es: (texto_pregunta, etiqueta_real)\n",
        "# etiquetas: \"vectorial\", \"tabular\", \"grafo\"\n",
        "\n",
        "datos_clasificacion = [\n",
        "    # VECTORIAL: manuales, FAQs, problemas de uso, reseñas\n",
        "    (\"¿Qué voltaje requiere la licuadora modelo P0001?\", \"vectorial\"),\n",
        "    (\"¿Cómo se limpia el filtro del lavarropas?\", \"vectorial\"),\n",
        "    (\"¿Este microondas tiene función grill?\", \"vectorial\"),\n",
        "    (\"Mi heladera hace un ruido raro, ¿es normal?\", \"vectorial\"),\n",
        "    (\"Mostrame las instrucciones para usar la procesadora\", \"vectorial\"),\n",
        "    (\"¿Cuánto mide el cable de este ventilador?\", \"vectorial\"),\n",
        "    (\"¿Cómo reinicio el horno si se queda trabado?\", \"vectorial\"),\n",
        "    (\"¿Qué garantía tiene el modelo P0003?\", \"vectorial\"),\n",
        "    (\"¿Dónde están las instrucciones de seguridad de la licuadora?\", \"vectorial\"),\n",
        "    (\"¿Cómo cambio el filtro de la aspiradora?\", \"vectorial\"),\n",
        "\n",
        "    # TABULAR: ventas, stock, precios, KPIs\n",
        "    (\"Mostrame las ventas totales del mes pasado\", \"tabular\"),\n",
        "    (\"¿Cuántas unidades de la heladera P0005 hay en stock?\", \"tabular\"),\n",
        "    (\"Listá los vendedores con más devoluciones\", \"tabular\"),\n",
        "    (\"¿Qué producto tuvo más tickets de soporte en 2024?\", \"tabular\"),\n",
        "    (\"Dame el top 5 de productos más vendidos\", \"tabular\"),\n",
        "    (\"Mostrame el total facturado por categoría este año\", \"tabular\"),\n",
        "    (\"¿Cuál fue la sucursal con más ventas este trimestre?\", \"tabular\"),\n",
        "    (\"Mostrame el stock actual de todos los lavarropas\", \"tabular\"),\n",
        "    (\"¿Qué vendedor tuvo más ventas de licuadoras?\", \"tabular\"),\n",
        "    (\"¿Cuántas devoluciones hubo de la procesadora P0010?\", \"tabular\"),\n",
        "\n",
        "    # GRAFO: compatibilidad entre productos/repuestos\n",
        "    (\"¿Qué repuestos son compatibles con la licuadora P0001?\", \"grafo\"),\n",
        "    (\"¿Este filtro es compatible con qué modelos de lavarropas?\", \"grafo\"),\n",
        "    (\"Listá todos los productos compatibles con el horno P0100\", \"grafo\"),\n",
        "    (\"¿Qué modelos comparten el mismo motor que la aspiradora P0200?\", \"grafo\"),\n",
        "    (\"¿Con qué otros productos es compatible la pieza X123?\", \"grafo\"),\n",
        "    (\"Mostrame todos los productos que comparten repuestos con la licuadora P0003\", \"grafo\"),\n",
        "    (\"¿Qué modelos usan el mismo filtro que la heladera P0300?\", \"grafo\"),\n",
        "    (\"¿La resistencia R45 es compatible con qué hornos?\", \"grafo\"),\n",
        "    (\"¿Qué otros modelos aceptan este mismo repuesto?\", \"grafo\"),\n",
        "    (\"Mostrame todos los productos relacionados por compatibilidad con P0500\", \"grafo\"),\n",
        "]\n",
        "\n",
        "df_clf = pd.DataFrame(datos_clasificacion, columns=[\"texto\", \"label\"])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ujz7mRAmTZB7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clasificador entrenado propio (TF-IDF + Logistic Regression)"
      ],
      "metadata": {
        "id": "2m3zgXIq0w0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Partición train / test\n",
        "X = df_clf[\"texto\"]\n",
        "y = df_clf[\"label\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "# 2) Vectorizador TF-IDF\n",
        "vectorizer_intencion = TfidfVectorizer(\n",
        "    ngram_range=(1, 2),   # unigrama + bigrama\n",
        "    min_df=1\n",
        ")\n",
        "\n",
        "X_train_vec = vectorizer_intencion.fit_transform(X_train)\n",
        "X_test_vec  = vectorizer_intencion.transform(X_test)\n",
        "\n",
        "# 3) Modelo de clasificación\n",
        "clf_intencion = LogisticRegression(\n",
        "    max_iter=200,\n",
        "    multi_class=\"auto\"\n",
        ")\n",
        "clf_intencion.fit(X_train_vec, y_train)\n",
        "\n",
        "# 4) Evaluación en test (modelo entrenado propio)\n",
        "y_pred = clf_intencion.predict(X_test_vec)\n",
        "\n",
        "print(\"=== CLASIFICADOR ENTRENADO (TF-IDF + LogisticRegression) ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "\n",
        "def clasificar_intencion_ml(pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Clasificador de intención basado en modelo entrenado (TF-IDF + LogisticRegression).\n",
        "\n",
        "    Devuelve una de las etiquetas:\n",
        "      - \"vectorial\"\n",
        "      - \"tabular\"\n",
        "      - \"grafo\"\n",
        "    \"\"\"\n",
        "    X_vec = vectorizer_intencion.transform([pregunta])\n",
        "    pred = clf_intencion.predict(X_vec)[0]\n",
        "    return pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmAoS06O0rNV",
        "outputId": "22d60883-ab0e-40a6-d5c0-45967175c92e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CLASIFICADOR ENTRENADO (TF-IDF + LogisticRegression) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       grafo       0.75      1.00      0.86         3\n",
            "     tabular       1.00      0.33      0.50         3\n",
            "   vectorial       0.75      1.00      0.86         3\n",
            "\n",
            "    accuracy                           0.78         9\n",
            "   macro avg       0.83      0.78      0.74         9\n",
            "weighted avg       0.83      0.78      0.74         9\n",
            "\n",
            "Matriz de confusión:\n",
            "[[3 0 0]\n",
            " [1 1 1]\n",
            " [0 0 3]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el clasificador entrenado se construyó un conjunto de **30 preguntas sintéticas**,\n",
        "distribuidas equitativamente en las tres clases de intención:\n",
        "\n",
        "- `vectorial`\n",
        "- `tabular`\n",
        "- `grafo`\n",
        "\n",
        "Cada pregunta representa un tipo de consulta que luego podría hacer un usuario real del sistema\n",
        "(por ejemplo, preguntas sobre uso de productos, sobre ventas/stock o sobre compatibilidad de repuestos).\n",
        "\n",
        "El pipeline utilizado fue:\n",
        "\n",
        "1. Vectorización de las preguntas con **TF-IDF**, considerando unigramas y bigramas.\n",
        "2. División del dataset en entrenamiento y prueba mediante `train_test_split`, manteniendo el balance entre clases.\n",
        "3. Entrenamiento de un modelo de **Logistic Regression**.\n",
        "4. Evaluación sobre el conjunto de prueba usando `classification_report` y `confusion_matrix`.\n",
        "\n",
        "En el conjunto de prueba se obtuvieron métricas cercanas a:\n",
        "\n",
        "- **Accuracy** ≈ 0.78  \n",
        "\n",
        "Este clasificador tiene como ventaja principal su **bajo costo de cómputo** y su independencia de servicios externos, por lo que es una buena opción cuando se prioriza eficiencia y simplicidad."
      ],
      "metadata": {
        "id": "DgqlVQyIGs-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clasificador basado en LLM (Few-Shot) y comparación"
      ],
      "metadata": {
        "id": "5Y3mSeXX1QFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def armar_prompt_clasificador_llm(pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Construye el prompt para que Gemini clasifique la intención\n",
        "    en una de las tres clases: vectorial / tabular / grafo.\n",
        "    \"\"\"\n",
        "\n",
        "    return f\"\"\"\n",
        "Sos un asistente que clasifica preguntas de usuarios de una empresa de electrodomésticos\n",
        "en TRES categorías según la FUENTE DE DATOS que habría que usar.\n",
        "\n",
        "Las clases posibles son:\n",
        "\n",
        "1) vectorial:\n",
        "   - Preguntas sobre cómo usar un producto.\n",
        "   - Consultas típicas de manuales, FAQs, problemas de uso, reseñas.\n",
        "   - Ejemplos: instrucciones, voltaje, limpieza, garantía, problemas de funcionamiento.\n",
        "\n",
        "2) tabular:\n",
        "   - Preguntas sobre datos numéricos o estadísticos.\n",
        "   - Ventas, stock, devoluciones, rankings, totales, top N, facturación, por sucursal.\n",
        "   - Ejemplos: \"top 10 más vendidos\", \"total facturado\", \"stock disponible\", \"vendedor con más ventas\".\n",
        "\n",
        "3) grafo:\n",
        "   - Preguntas sobre compatibilidad entre productos o repuestos.\n",
        "   - Qué modelos comparten componentes, qué repuestos sirven para qué modelos.\n",
        "   - Ejemplos: \"¿Con qué modelos es compatible?\", \"¿qué productos comparten el mismo motor?\".\n",
        "\n",
        "EJEMPLOS (few-shot):\n",
        "\n",
        "PREGUNTA: \"¿Qué voltaje requiere la licuadora modelo P0001?\"\n",
        "CLASE: vectorial\n",
        "\n",
        "PREGUNTA: \"¿Cómo se limpia el filtro del lavarropas?\"\n",
        "CLASE: vectorial\n",
        "\n",
        "PREGUNTA: \"Mostrame las ventas totales del mes pasado\"\n",
        "CLASE: tabular\n",
        "\n",
        "PREGUNTA: \"¿Cuántas unidades de la heladera P0005 hay en stock?\"\n",
        "CLASE: tabular\n",
        "\n",
        "PREGUNTA: \"¿Qué repuestos son compatibles con la licuadora P0001?\"\n",
        "CLASE: grafo\n",
        "\n",
        "PREGUNTA: \"Mostrame todos los productos que comparten repuestos con la licuadora P0003\"\n",
        "CLASE: grafo\n",
        "\n",
        "\n",
        "Ahora clasificá la siguiente pregunta del usuario EN UNA SOLA PALABRA,\n",
        "usando exactamente una de estas opciones:\n",
        "\n",
        "- vectorial\n",
        "- tabular\n",
        "- grafo\n",
        "\n",
        "No agregues explicaciones ni texto extra.\n",
        "\n",
        "PREGUNTA A CLASIFICAR:\n",
        "{pregunta}\n",
        "\n",
        "RESPUESTA:\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def clasificar_intencion_llm(pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Clasificador de intención basado en LLM (Gemini) con few-shot prompting.\n",
        "\n",
        "    Devuelve:\n",
        "      - \"vectorial\"\n",
        "      - \"tabular\"\n",
        "      - \"grafo\"\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = armar_prompt_clasificador_llm(pregunta)\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=modelo_gemini,\n",
        "        contents=[prompt]\n",
        "    )\n",
        "\n",
        "    texto = response.text.strip().lower()\n",
        "\n",
        "    # Normalizamos por si el modelo devuelve algo tipo \"La clase es: tabular\"\n",
        "    if \"vectorial\" in texto:\n",
        "        return \"vectorial\"\n",
        "    if \"tabular\" in texto:\n",
        "        return \"tabular\"\n",
        "    if \"grafo\" in texto:\n",
        "        return \"grafo\"\n",
        "\n",
        "    # Fallback: si no podemos parsear, usamos el modelo entrenado\n",
        "    return clasificar_intencion_ml(pregunta)\n"
      ],
      "metadata": {
        "id": "k0DXkQGL04gx"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparación de clasificadores"
      ],
      "metadata": {
        "id": "pK72A3ln1IKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def comparar_clasificadores(df=df_clf, max_muestras_llm: int = 20):\n",
        "    \"\"\"\n",
        "    Compara el clasificador entrenado (ML) y el clasificador LLM few-shot.\n",
        "\n",
        "    - Para el modelo entrenado se calcula classification_report completo.\n",
        "    - Para el LLM se evalúa sobre un subconjunto (max_muestras_llm) para\n",
        "      evitar demasiadas llamadas a la API.\n",
        "\n",
        "    Imprime las métricas por pantalla.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Modelo entrenado (ML) ya lo evaluamos arriba con X_test e y_test ---\n",
        "    print(\"\\n\\n=== RESUMEN: CLASIFICADOR ENTRENADO (ML) ===\")\n",
        "    X_vec = vectorizer_intencion.transform(df[\"texto\"])\n",
        "    y_true = df[\"label\"]\n",
        "    y_pred_ml = clf_intencion.predict(X_vec)\n",
        "    print(classification_report(y_true, y_pred_ml))\n",
        "\n",
        "    # --- Clasificador LLM (few-shot) en un subconjunto ---\n",
        "    print(\"\\n=== RESUMEN: CLASIFICADOR LLM (FEW-SHOT) ===\")\n",
        "    # Subconjunto para no gastar demasiadas llamadas\n",
        "    sub = df.sample(min(max_muestras_llm, len(df)), random_state=42)\n",
        "\n",
        "    y_true_llm = []\n",
        "    y_pred_llm = []\n",
        "\n",
        "    for _, row in sub.iterrows():\n",
        "        txt = row[\"texto\"]\n",
        "        etiqueta_real = row[\"label\"]\n",
        "        pred_llm = clasificar_intencion_llm(txt)\n",
        "\n",
        "        y_true_llm.append(etiqueta_real)\n",
        "        y_pred_llm.append(pred_llm)\n",
        "\n",
        "        print(f\"Pregunta: {txt}\")\n",
        "        print(f\"  Real: {etiqueta_real} | LLM: {pred_llm}\")\n",
        "        print(\"---\")\n",
        "\n",
        "    print(\"\\n=== MÉTRICAS LLM EN SUBCONJUNTO ===\")\n",
        "    print(classification_report(y_true_llm, y_pred_llm))"
      ],
      "metadata": {
        "id": "mt9Ozlut1FwT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funcion unificada para el asistente"
      ],
      "metadata": {
        "id": "5zCMvoT706lL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clasificar_intencion(query_usuario: str, metodo: str = \"ml\") -> str:\n",
        "    \"\"\"\n",
        "    Envuelve ambos clasificadores y te deja elegir el método:\n",
        "\n",
        "    metodo = \"ml\"   -> usa clasificador entrenado (TF-IDF + LogisticRegression)\n",
        "    metodo = \"llm\"  -> usa clasificador basado en LLM few-shot\n",
        "    metodo = \"mixto\"-> usa ML y, si quisiera, podría combinar (acá por ahora = ML)\n",
        "    \"\"\"\n",
        "    metodo = metodo.lower()\n",
        "\n",
        "    if metodo == \"llm\":\n",
        "        return clasificar_intencion_llm(query_usuario)\n",
        "    return clasificar_intencion_ml(query_usuario)\n"
      ],
      "metadata": {
        "id": "7oQvRA2y1A-8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def asistente_electro(\n",
        "    query_usuario: str,\n",
        "    historial=None,\n",
        "    k_vectorial: int = 5,\n",
        "    metodo_clasificador: str = \"ml\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Punto de entrada único del asistente.\n",
        "    - Usa el clasificador de intención (ML o LLM)\n",
        "    - Llama al pipeline correspondiente (vectorial, tabular o grafo)\n",
        "    \"\"\"\n",
        "\n",
        "    fuente = clasificar_intencion(query_usuario, metodo=metodo_clasificador)\n",
        "\n",
        "    if fuente == \"vectorial\":\n",
        "\n",
        "        return pipeline_vectorial(query_usuario, k_final=k_vectorial)\n",
        "\n",
        "        # # Si preferís algo más simple por ahora:\n",
        "        # fragmentos = buscar_vectorial(query_usuario, k=k_vectorial)\n",
        "        # respuesta = responder_desde_vectorial_con_llm(query_usuario, fragmentos, client)\n",
        "        # return {\n",
        "        #     \"fuente\": \"vectorial\",\n",
        "        #     \"fragmentos\": fragmentos,\n",
        "        #     \"respuesta\": respuesta,\n",
        "        # }\n",
        "\n",
        "    elif fuente == \"tabular\":\n",
        "        return pipeline_tabular(query_usuario)\n",
        "\n",
        "    elif fuente == \"grafo\":\n",
        "        return pipeline_grafo(query_usuario)\n",
        "\n",
        "    # Fallback raro, por las dudas\n",
        "    return {\n",
        "        \"fuente\": \"desconocida\",\n",
        "        \"respuesta\": (\n",
        "            \"Por ahora no sé a qué fuente de datos dirigir esta consulta. \"\n",
        "            \"Probá reformular la pregunta con más detalle sobre si buscás precios, \"\n",
        "            \"compatibilidades o instrucciones de uso.\"\n",
        "        )\n",
        "    }\n"
      ],
      "metadata": {
        "id": "i2qQBdH93Eav"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de uso del asistente integrado\n",
        "\n",
        "resp = asistente_electro(\"¿Qué productos son compatibles con la Procesadora?\")\n",
        "print(resp[\"fuente\"])\n",
        "print()\n",
        "print(resp[\"respuesta\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwPK3cpsEq_c",
        "outputId": "86c20006-be15-4a35-ca80-d654acc28ad8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Cypher generado por el modelo:\n",
            "\n",
            "MATCH (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto) WHERE p.nombre = \"Procesadora\" RETURN c.id AS id_producto, c.nombre AS nombre, c.categoria AS categoria, c.marca AS marca, r.comparte AS componente_compartido\n",
            "\n",
            "---\n",
            "\n",
            "grafo\n",
            "\n",
            "Los siguientes productos son compatibles con la Procesadora, ya que comparten componentes:\n",
            "\n",
            "*   **Advanced Batidora de Pie:** Comparte el componente \"Accesorios\".\n",
            "*   **Profesional Abridor de Latas:** Comparte el componente \"Jarra\".\n",
            "*   **Turbo Exprimidor:** Comparte el componente \"Accesorios\".\n",
            "*   **Olla de Cocción Lenta:** Comparte el componente \"Jarra\".\n",
            "*   **Turbo Microondas:** Comparte el componente \"Panel de control\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comparar_clasificadores()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gCXi4BSDS7-",
        "outputId": "a4697933-699b-4f0d-d4e1-b7251841f814"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== RESUMEN: CLASIFICADOR ENTRENADO (ML) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       grafo       0.91      1.00      0.95        10\n",
            "     tabular       1.00      0.80      0.89        10\n",
            "   vectorial       0.91      1.00      0.95        10\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.94      0.93      0.93        30\n",
            "weighted avg       0.94      0.93      0.93        30\n",
            "\n",
            "\n",
            "=== RESUMEN: CLASIFICADOR LLM (FEW-SHOT) ===\n",
            "Pregunta: ¿La resistencia R45 es compatible con qué hornos?\n",
            "  Real: grafo | LLM: grafo\n",
            "---\n",
            "Pregunta: Mostrame el total facturado por categoría este año\n",
            "  Real: tabular | LLM: tabular\n",
            "---\n",
            "Pregunta: ¿Qué modelos comparten el mismo motor que la aspiradora P0200?\n",
            "  Real: grafo | LLM: grafo\n",
            "---\n",
            "Pregunta: Mostrame el stock actual de todos los lavarropas\n",
            "  Real: tabular | LLM: tabular\n",
            "---\n",
            "Pregunta: ¿Dónde están las instrucciones de seguridad de la licuadora?\n",
            "  Real: vectorial | LLM: vectorial\n",
            "---\n",
            "Pregunta: ¿Cómo cambio el filtro de la aspiradora?\n",
            "  Real: vectorial | LLM: vectorial\n",
            "---\n",
            "Pregunta: ¿Qué otros modelos aceptan este mismo repuesto?\n",
            "  Real: grafo | LLM: grafo\n",
            "---\n",
            "Pregunta: ¿Con qué otros productos es compatible la pieza X123?\n",
            "  Real: grafo | LLM: grafo\n",
            "---\n",
            "Pregunta: Listá los vendedores con más devoluciones\n",
            "  Real: tabular | LLM: tabular\n",
            "---\n",
            "Pregunta: ¿Qué voltaje requiere la licuadora modelo P0001?\n",
            "  Real: vectorial | LLM: vectorial\n",
            "---\n",
            "Pregunta: Mostrame las instrucciones para usar la procesadora\n",
            "  Real: vectorial | LLM: vectorial\n",
            "---\n",
            "Pregunta: ¿Cuál fue la sucursal con más ventas este trimestre?\n",
            "  Real: tabular | LLM: tabular\n",
            "---\n",
            "Pregunta: ¿Cuánto mide el cable de este ventilador?\n",
            "  Real: vectorial | LLM: vectorial\n",
            "---\n",
            "Pregunta: ¿Qué producto tuvo más tickets de soporte en 2024?\n",
            "  Real: tabular | LLM: tabular\n",
            "---\n",
            "Pregunta: ¿Cuántas unidades de la heladera P0005 hay en stock?\n",
            "  Real: tabular | LLM: tabular\n",
            "---\n",
            "Pregunta: Listá todos los productos compatibles con el horno P0100\n",
            "  Real: grafo | LLM: grafo\n",
            "---\n",
            "Pregunta: ¿Cómo se limpia el filtro del lavarropas?\n",
            "  Real: vectorial | LLM: vectorial\n",
            "---\n",
            "Pregunta: ¿Este microondas tiene función grill?\n",
            "  Real: vectorial | LLM: vectorial\n",
            "---\n",
            "Pregunta: Mostrame todos los productos que comparten repuestos con la licuadora P0003\n",
            "  Real: grafo | LLM: grafo\n",
            "---\n",
            "Pregunta: Mi heladera hace un ruido raro, ¿es normal?\n",
            "  Real: vectorial | LLM: vectorial\n",
            "---\n",
            "\n",
            "=== MÉTRICAS LLM EN SUBCONJUNTO ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       grafo       1.00      1.00      1.00         6\n",
            "     tabular       1.00      1.00      1.00         6\n",
            "   vectorial       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Además del modelo entrenado tradicional, se implementó un **clasificador de intención basado en LLM**\n",
        "utilizando Gemini. Para esto se diseñó un prompt que:\n",
        "\n",
        "- Describe en lenguaje natural las tres clases posibles (`vectorial`, `tabular`, `grafo`).\n",
        "- Incluye **ejemplos etiquetados** de preguntas típicas de cada clase (enfoque *few-shot*).\n",
        "- Le indica explícitamente al modelo que debe responder **solo con el nombre de la clase**.\n",
        "\n",
        "A partir de este prompt, la función `clasificar_intencion_llm` toma una pregunta en lenguaje natural,\n",
        "consulta al modelo y normaliza su salida para obtener una de las tres etiquetas.\n",
        "\n",
        "Para comparar ambos enfoques se utilizó la función `comparar_clasificadores`, que evalúa:\n",
        "\n",
        "- El **clasificador entrenado (ML)** sobre todo el dataset sintético.\n",
        "- El **clasificador LLM** sobre un subconjunto de ejemplos, para evitar un número excesivo de llamadas a la API.\n",
        "\n",
        "En los experimentos realizados se observaron resultados del estilo:\n",
        "\n",
        "- Clasificador entrenado (ML):  \n",
        "  - accuracy ≈ 0.93  \n",
        "  - buen F1-score en las tres clases\n",
        "\n",
        "- Clasificador LLM (few-shot):  \n",
        "  - accuracy ≈ 1.00 en el subconjunto evaluado  \n",
        "  - clasificación correcta en todos los ejemplos probados\n",
        "\n",
        "Estos resultados muestran que el LLM, guiado con ejemplos adecuados, es capaz de captar muy bien\n",
        "la diferencia semántica entre las tres intenciones. Sin embargo, su uso implica:\n",
        "\n",
        "- **Mayor latencia** (cada predicción requiere una llamada a la API).\n",
        "- **Costo asociado** al uso del modelo en la nube.\n",
        "- Dependencia de la disponibilidad del servicio externo.\n",
        "\n",
        "Por otro lado, el clasificador entrenado con TF-IDF + Logistic Regression ofrece:\n",
        "\n",
        "- Muy buen rendimiento (más del 90% de accuracy).\n",
        "- Costo de cómputo despreciable una vez entrenado.\n",
        "- Independencia de APIs externas.\n",
        "\n",
        "Por este motivo, en la implementación final del asistente se ofrece la posibilidad de usar\n",
        "ambos clasificadores a través del parámetro `metodo_clasificador`, pero se deja como opción\n",
        "por defecto el **clasificador entrenado (ML)**, priorizando eficiencia y simplicidad en el\n",
        "entorno de producción, y manteniendo el clasificador LLM como alternativa más “inteligente”\n",
        "pero también más costosa."
      ],
      "metadata": {
        "id": "05oHYIfMHobs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recuperación Híbrida (BM25 + Embeddings)\n",
        "\n",
        "Para mejorar la calidad de la recuperación en la fuente vectorial, se implementa un\n",
        "pipeline híbrido que combina dos enfoques complementarios:\n",
        "\n",
        "1. **Búsqueda por palabras clave (BM25)**  \n",
        "   Permite detectar coincidencias literales y términos específicos, siendo muy útil cuando\n",
        "   el usuario menciona palabras exactas del manual o la FAQ.\n",
        "\n",
        "2. **Búsqueda semántica mediante embeddings**  \n",
        "   Utiliza el espacio vectorial generado previamente, detectando similitud semántica\n",
        "   incluso cuando no se repiten las mismas palabras.\n",
        "\n",
        "La combinación de ambos métodos permite cubrir casos donde:\n",
        "\n",
        "- El usuario recuerda parte de una frase del manual (BM25 funciona bien).\n",
        "- El usuario parafrasea o hace una pregunta conceptual no textual (embeddings funcionan mejor)."
      ],
      "metadata": {
        "id": "nkNI6x5zxWqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus de texto para BM25 (uno por chunk)\n",
        "bm25_corpus = [c[\"content\"] for c in chunks_final]\n",
        "\n",
        "# Mapeo índice entero -> chunk original\n",
        "bm25_id_to_chunk = chunks_final  # lista, índice = entero usado por BM25\n"
      ],
      "metadata": {
        "id": "xI_CpFbtxZn6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Búsqueda por palabras clave con BM25\n",
        "\n",
        "Se construyó un índice BM25 utilizando la librería `txtai`, donde cada documento\n",
        "representa un *chunk* de los manuales, reseñas y FAQs.\n",
        "\n",
        "El objetivo de esta etapa es recuperar rápidamente candidatos que contienen términos\n",
        "mencionados por el usuario. La función `buscar_bm25()` recibe una consulta y devuelve\n",
        "los `k` chunks más relevantes según BM25.\n",
        "\n",
        "BM25 es especialmente útil cuando el usuario utiliza términos específicos del manual o\n",
        "palabras que aparecen textualmente en los documentos."
      ],
      "metadata": {
        "id": "RaXYYNfyL9QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from txtai.scoring import ScoringFactory\n",
        "\n",
        "# Índice BM25\n",
        "bm25_scoring = ScoringFactory.create({\n",
        "    \"method\": \"bm25\",\n",
        "    \"terms\": True  # índice de palabras clave\n",
        "})\n",
        "\n",
        "# Indexar todos los chunks\n",
        "bm25_scoring.index(\n",
        "    ( (idx, text, None) for idx, text in enumerate(bm25_corpus) )\n",
        ")\n",
        "\n",
        "print(\"Índice BM25 construido. Cantidad de documentos:\", bm25_scoring.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNmgysOAxsoT",
        "outputId": "9bba0b01-12b2-4421-9005-ff132820ec78"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índice BM25 construido. Cantidad de documentos: 10839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_bm25(consulta: str, k: int = 10):\n",
        "    \"\"\"\n",
        "    Búsqueda por palabras clave usando BM25 sobre los chunks.\n",
        "    Devuelve una lista de dicts con:\n",
        "    - id, content, metadata, score_bm25\n",
        "    \"\"\"\n",
        "    resultados_raw = bm25_scoring.search(consulta, k)\n",
        "\n",
        "    resultados = []\n",
        "    for idx, score in resultados_raw:\n",
        "        chunk = bm25_id_to_chunk[idx]\n",
        "        resultados.append({\n",
        "            \"id\": chunk[\"id\"],\n",
        "            \"content\": chunk[\"content\"],\n",
        "            \"metadata\": chunk[\"metadata\"],\n",
        "            \"score_bm25\": float(score),\n",
        "        })\n",
        "\n",
        "    return resultados\n"
      ],
      "metadata": {
        "id": "pIKkRvvZx00D"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def _normalizar_scores(valores):\n",
        "    \"\"\"\n",
        "    Normaliza una lista de valores a [0, 1].\n",
        "    Si todos son iguales, devuelve 1.0 para todos.\n",
        "    \"\"\"\n",
        "    if not valores:\n",
        "        return []\n",
        "\n",
        "    vmin = min(valores)\n",
        "    vmax = max(valores)\n",
        "\n",
        "    if math.isclose(vmin, vmax):\n",
        "        return [1.0 for _ in valores]\n",
        "\n",
        "    return [(v - vmin) / (vmax - vmin) for v in valores]\n"
      ],
      "metadata": {
        "id": "oJBRkCWMx1ar"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Búsqueda Híbrida: combinación BM25 + búsqueda semántica\n",
        "\n",
        "La función `buscar_hibrida()` fusiona los resultados de BM25 y la búsqueda vectorial\n",
        "semántica. Para cada chunk candidato se calculan dos puntajes:\n",
        "\n",
        "- `score_bm25_norm`: puntaje normalizado de BM25\n",
        "- `score_sem_norm`: puntaje normalizado de la similitud semántica\n",
        "\n",
        "Luego se combinan mediante un hiperparámetro `alpha`.\n",
        "\n",
        "Este mecanismo garantiza que:\n",
        "\n",
        "- Si el usuario usa palabras exactas → BM25 aporta mayor peso.\n",
        "- Si el usuario expresa una idea más conceptual → embeddings compensan mejor.\n",
        "\n",
        "Finalmente se ordenan los chunks según el score híbrido y se devuelven los mejores candidatos."
      ],
      "metadata": {
        "id": "OYyCzuAFMZ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_hibrida(\n",
        "    consulta: str,\n",
        "    k_bm25: int = 20,\n",
        "    k_sem: int = 20,\n",
        "    k_final: int = 10,\n",
        "    alpha: float = 0.5,\n",
        "    filtros: dict | None = None,\n",
        "    devolver_embeddings: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Búsqueda híbrida que combina:\n",
        "    - BM25 (palabras clave)\n",
        "    - Búsqueda vectorial (embeddings en Chroma)\n",
        "\n",
        "    alpha controla el peso de BM25:\n",
        "      score_hibrido = alpha * score_bm25_norm + (1 - alpha) * score_sem_norm\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) BM25\n",
        "    res_bm25 = buscar_bm25(consulta, k=k_bm25)\n",
        "\n",
        "    # 2) Búsqueda vectorial (ya existente)\n",
        "    res_vec = buscar_vectorial(\n",
        "        consulta,\n",
        "        k=k_sem,\n",
        "        filtros=filtros,\n",
        "        devolver_embeddings=devolver_embeddings\n",
        "    )\n",
        "\n",
        "    # 3) Armar diccionario combinado por id de chunk\n",
        "    combinado = {}\n",
        "\n",
        "    # BM25\n",
        "    for r in res_bm25:\n",
        "        cid = r[\"id\"]\n",
        "        combinado.setdefault(cid, {\n",
        "            \"id\": cid,\n",
        "            \"content\": r[\"content\"],\n",
        "            \"metadata\": r[\"metadata\"],\n",
        "            \"score_bm25\": None,\n",
        "            \"score_sem\": None,\n",
        "            \"score_hibrido\": None,\n",
        "        })\n",
        "        combinado[cid][\"score_bm25\"] = r[\"score_bm25\"]\n",
        "\n",
        "    # Vectorial: usamos -distance como “score crudo” (menor distancia = mejor → mayor score)\n",
        "    for r in res_vec:\n",
        "        cid = r[\"id\"]\n",
        "        combinado.setdefault(cid, {\n",
        "            \"id\": cid,\n",
        "            \"content\": r[\"content\"],\n",
        "            \"metadata\": r[\"metadata\"],\n",
        "            \"score_bm25\": None,\n",
        "            \"score_sem\": None,\n",
        "            \"score_hibrido\": None,\n",
        "        })\n",
        "        # guardamos también por si querés usar distance directo\n",
        "        combinado[cid][\"distance\"] = r[\"distance\"]\n",
        "        combinado[cid][\"score_sem_raw\"] = -float(r[\"distance\"])\n",
        "        if devolver_embeddings and \"embedding_doc\" in r:\n",
        "            combinado[cid][\"embedding_doc\"] = r[\"embedding_doc\"]\n",
        "\n",
        "    # 4) Normalizar scores\n",
        "    # BM25\n",
        "    scores_bm25 = [\n",
        "        v[\"score_bm25\"]\n",
        "        for v in combinado.values()\n",
        "        if v[\"score_bm25\"] is not None\n",
        "    ]\n",
        "    scores_sem_raw = [\n",
        "        v[\"score_sem_raw\"]\n",
        "        for v in combinado.values()\n",
        "        if \"score_sem_raw\" in v\n",
        "    ]\n",
        "\n",
        "    scores_bm25_norm = _normalizar_scores(scores_bm25)\n",
        "    scores_sem_norm = _normalizar_scores(scores_sem_raw)\n",
        "\n",
        "    # Mapear normalizados de vuelta a las entradas\n",
        "    # (hacemos listas paralelas para simplificar)\n",
        "    i_bm25 = 0\n",
        "    for v in combinado.values():\n",
        "        if v[\"score_bm25\"] is not None:\n",
        "            v[\"score_bm25_norm\"] = scores_bm25_norm[i_bm25]\n",
        "            i_bm25 += 1\n",
        "        else:\n",
        "            v[\"score_bm25_norm\"] = 0.0\n",
        "\n",
        "    i_sem = 0\n",
        "    for v in combinado.values():\n",
        "        if \"score_sem_raw\" in v:\n",
        "            v[\"score_sem_norm\"] = scores_sem_norm[i_sem]\n",
        "            i_sem += 1\n",
        "        else:\n",
        "            v[\"score_sem_norm\"] = 0.0\n",
        "\n",
        "    # 5) Score híbrido\n",
        "    for v in combinado.values():\n",
        "        sb = v.get(\"score_bm25_norm\", 0.0)\n",
        "        ss = v.get(\"score_sem_norm\", 0.0)\n",
        "        v[\"score_hibrido\"] = alpha * sb + (1 - alpha) * ss\n",
        "\n",
        "    # 6) Ordenar y devolver top-k final\n",
        "    lista = list(combinado.values())\n",
        "    lista.sort(key=lambda x: x[\"score_hibrido\"], reverse=True)\n",
        "\n",
        "    return lista[:k_final]"
      ],
      "metadata": {
        "id": "8ZaoaO9QMSGP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-Ranking con CrossEncoder\n",
        "\n",
        "Luego de obtener los candidatos híbridos, se aplica una etapa de **re-ranking** utilizando\n",
        "un modelo CrossEncoder (`ms-marco-MiniLM-L-2-v2`).  \n",
        "A diferencia de los embeddings, el CrossEncoder analiza la consulta y el documento juntos,\n",
        "permitiendo evaluar relaciones más profundas entre ambos.\n",
        "\n",
        "El proceso consiste en:\n",
        "\n",
        "1. Tomar los candidatos producidos por la búsqueda híbrida.\n",
        "2. Evaluar cada par *(consulta, chunk)* con el CrossEncoder.\n",
        "3. Ordenarlos nuevamente según este puntaje más preciso.\n",
        "\n",
        "Este re-ranking mejora fuertemente la calidad de los primeros resultados (top-k),\n",
        "siguiendo las recomendaciones estándar en sistemas RAG profesionales."
      ],
      "metadata": {
        "id": "HF59xGXIx9X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from txtai.pipeline import Similarity\n",
        "\n",
        "# Modelo de CrossEncoder para rerank\n",
        "crossencoder = Similarity(\n",
        "    \"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n",
        "    crossencode=True,\n",
        "    gpu=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cafvdywx_LQ",
        "outputId": "6a7c3d8b-c970-438f-a1b4-04264bc02102"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rerank_crossencoder(consulta: str, resultados: list[dict], top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Reordena los resultados usando un modelo cross-encoder.\n",
        "    Toma:\n",
        "      - consulta (texto)\n",
        "      - resultados: lista de dicts con al menos 'content'\n",
        "      - top_k: cuántos devolver\n",
        "\n",
        "    Devuelve:\n",
        "      - lista de dicts con campo extra 'score_rerank'\n",
        "    \"\"\"\n",
        "    if not resultados:\n",
        "        return []\n",
        "\n",
        "    textos = [r[\"content\"] for r in resultados]\n",
        "\n",
        "    # Similarity devuelve lista de (id, score), ordenada por score desc\n",
        "    pares = crossencoder(consulta, textos)\n",
        "\n",
        "    rerankeados = []\n",
        "    for idx, score in pares[:top_k]:\n",
        "        item = resultados[idx].copy()\n",
        "        item[\"score_rerank\"] = float(score)\n",
        "        rerankeados.append(item)\n",
        "\n",
        "    return rerankeados\n"
      ],
      "metadata": {
        "id": "kXewYu5EyBvC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline Vectorial Avanzado\n",
        "\n",
        "Se define un pipeline completo para la fuente VECTORIAL:\n",
        "\n",
        "1. **Búsqueda híbrida (BM25 + embeddings)**\n",
        "2. **Re-ranking mediante CrossEncoder**\n",
        "3. **Construcción de la respuesta final con un LLM**, usando como contexto solo los\n",
        "   `k` chunks mejor posicionados.\n",
        "\n",
        "Esta arquitectura representa un pipeline RAG consistente con sistemas de producción,\n",
        "donde se combinan técnicas de recuperación lexical, recuperación semántica y\n",
        "re-ranking profundo.\n",
        "\n",
        "El resultado es un sistema robusto, capaz de recuperar información precisa incluso\n",
        "cuando el usuario formula preguntas incompletas, vagas o parafraseadas.\n"
      ],
      "metadata": {
        "id": "GEpmfZcSyPgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def extraer_producto_id_desde_query(query: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Busca un código de producto tipo P0001, P0199, etc. en el texto.\n",
        "    Si no encuentra, devuelve None.\n",
        "    \"\"\"\n",
        "    match = re.search(r\"\\bP\\d{4}\\b\", query)\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    return None\n",
        "\n",
        "\n",
        "def pipeline_vectorial(\n",
        "    query_usuario: str,\n",
        "    k_bm25: int = 20,\n",
        "    k_sem: int = 20,\n",
        "    k_final: int = 5,\n",
        "    alpha: float = 0.5,\n",
        "    filtros: dict | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline completo para la fuente VECTORIAL:\n",
        "    1) Recupera candidatos con búsqueda híbrida (BM25 + embeddings).\n",
        "    2) (Opcional) Filtra por producto si se detecta un código P#### en la query.\n",
        "    3) Re-rankea los candidatos con un CrossEncoder.\n",
        "    4) Genera respuesta en lenguaje natural con el LLM.\n",
        "    \"\"\"\n",
        "\n",
        "    # 0) Partimos de que NO hay filtros, salvo que el llamador nos haya pasado alguno\n",
        "    filtros_final = filtros.copy() if filtros else None\n",
        "\n",
        "    # 1) Intentar detectar un código de producto en la pregunta (P0001, P0199, etc.)\n",
        "    producto_id = extraer_producto_id_desde_query(query_usuario)\n",
        "\n",
        "    # Si detectamos producto_id, preparamos un filtro para la parte vectorial (Chroma)\n",
        "    if producto_id is not None:\n",
        "        filtros_producto = {\n",
        "            \"$or\": [\n",
        "                {\"producto_id\": producto_id},\n",
        "                {\"id_producto\": producto_id},\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        if filtros_final:\n",
        "            filtros_final = {\"$and\": [filtros_final, filtros_producto]}\n",
        "        else:\n",
        "            filtros_final = filtros_producto  # acá sí hay un filtro real\n",
        "\n",
        "    # 2) Búsqueda híbrida (BM25 + vectorial)\n",
        "    # Pedimos varios candidatos para que el rerank tenga de dónde elegir\n",
        "    k_candidatos = max(k_final * 5, 25)\n",
        "\n",
        "    candidatos = buscar_hibrida(\n",
        "        consulta=query_usuario,\n",
        "        k_bm25=k_bm25,\n",
        "        k_sem=k_sem,\n",
        "        k_final=k_candidatos,\n",
        "        alpha=alpha,\n",
        "        filtros=filtros_final,      # IMPORTANTE: puede ser dict o None, pero nunca {}\n",
        "        devolver_embeddings=False,\n",
        "    )\n",
        "\n",
        "    # 3) Filtro post-hoc por producto_id en metadatos (para limpiar lo que vino de BM25)\n",
        "    if producto_id is not None:\n",
        "        candidatos_filtrados = [\n",
        "            c for c in candidatos\n",
        "            if c[\"metadata\"].get(\"producto_id\") == producto_id\n",
        "            or c[\"metadata\"].get(\"id_producto\") == producto_id\n",
        "        ]\n",
        "        if candidatos_filtrados:\n",
        "            candidatos = candidatos_filtrados\n",
        "\n",
        "    # 4) Re-rank con CrossEncoder sobre todos los candidatos restantes\n",
        "    rerankeados = rerank_crossencoder(\n",
        "        consulta=query_usuario,\n",
        "        resultados=candidatos,\n",
        "        top_k=k_final,\n",
        "    )\n",
        "\n",
        "    # Si por algún motivo el rerank falla o da vacío, usamos los candidatos originales\n",
        "    fragmentos_finales = rerankeados if rerankeados else candidatos[:k_final]\n",
        "\n",
        "    # 5) Respuesta con LLM usando estos fragmentos\n",
        "    respuesta = responder_desde_vectorial_con_llm(\n",
        "        query_usuario,\n",
        "        fragmentos_finales,\n",
        "        client,\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"fuente\": \"vectorial\",\n",
        "        \"fragmentos\": fragmentos_finales,\n",
        "        \"respuesta\": respuesta,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "6OXLZjmjeJE_"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Asistente integrado con búsqueda híbrida y ReRank\n",
        "\n",
        "A partir de los componentes anteriores (BM25, búsqueda semántica, búsqueda híbrida\n",
        "y re-ranqueo con CrossEncoder), se define una versión avanzada del asistente que\n",
        "integra:\n",
        "\n",
        "- Clasificador de intención (ML o LLM)\n",
        "- Pipeline híbrido para la fuente vectorial\n",
        "- Pipelines dinámicos para datos tabulares y grafo"
      ],
      "metadata": {
        "id": "Oa3Sjwf5KfSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def asistente_electro_avanzado(\n",
        "    query_usuario: str,\n",
        "    historial=None,\n",
        "    k_vectorial: int = 5,\n",
        "    metodo_clasificador: str = \"ml\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Versión AVANZADA del asistente.\n",
        "\n",
        "    Diferencias con asistente_electro (básico):\n",
        "    - Para la fuente VECTORIAL usa el pipeline híbrido + rerank:\n",
        "        pipeline_vectorial(...)\n",
        "    - Para TABULAR y GRAFO reutiliza los mismos pipelines dinámicos.\n",
        "\n",
        "    El clasificador de intención puede ser:\n",
        "    - \"ml\"  -> clasificador entrenado TF-IDF + LogisticRegression\n",
        "    - \"llm\" -> clasificador basado en Gemini few-shot\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Clasificar la intención de la consulta\n",
        "    fuente = clasificar_intencion(query_usuario, metodo=metodo_clasificador)\n",
        "\n",
        "    # 2) Elegir pipeline según la fuente\n",
        "    if fuente == \"vectorial\":\n",
        "        # Ahora sí usamos la búsqueda híbrida + rerank + RAG\n",
        "        resultado = pipeline_vectorial(\n",
        "            query_usuario,\n",
        "            k_bm25=20,\n",
        "            k_sem=20,\n",
        "            k_final=k_vectorial,\n",
        "            alpha=0.5,\n",
        "            filtros=None,\n",
        "        )\n",
        "        return resultado\n",
        "\n",
        "    elif fuente == \"tabular\":\n",
        "        return pipeline_tabular(query_usuario)\n",
        "\n",
        "    elif fuente == \"grafo\":\n",
        "        return pipeline_grafo(query_usuario)\n",
        "\n",
        "    # Fallback por las dudas\n",
        "    return {\n",
        "        \"fuente\": \"desconocida\",\n",
        "        \"respuesta\": (\n",
        "            \"No pude determinar a qué fuente de datos dirigir esta consulta. \"\n",
        "            \"Probá reformular la pregunta indicando si buscás datos numéricos, \"\n",
        "            \"instrucciones de uso o compatibilidad entre productos.\"\n",
        "        )\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AG8yHZShKQbb"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Elección del LLM y Justificación del Entorno de Ejecución\n",
        "\n",
        "Para este Trabajo Práctico se utiliza un modelo de lenguaje alojado **en la nube**, específicamente\n",
        "el modelo **Gemini** de Google. La elección de un modelo en la nube se justifica por los\n",
        "siguientes motivos:\n",
        "\n",
        "1. **Limitaciones computacionales locales:**  \n",
        "   Los modelos LLM modernos requieren GPU potentes y memoria significativa.\n",
        "   Ejecutarlos localmente en un entorno como Google Colab o notebooks personales no es viable sin hardware especializado.\n",
        "\n",
        "2. **Disponibilidad inmediata y sin configuración:**  \n",
        "   El acceso a Gemini a través de API permite utilizar modelos avanzados sin necesidad de instalar pesos, configurar entornos complejos o gestionar almacenamiento.\n",
        "\n",
        "3. **Mayor calidad y robustez:**  \n",
        "   Los modelos en la nube suelen ser más grandes, más actualizados y con mejor\n",
        "   rendimiento que los modelos locales livianos (ej: GPT2, LLaMA 7B, Mistral 7B).\n",
        "   Esto es crucial para tareas como:\n",
        "   - generación de código,\n",
        "   - comprensión semántica,\n",
        "   - clasificación de intención,\n",
        "   - generación de texto en lenguaje natural.\n",
        "\n",
        "4. **Escalabilidad y estabilidad:**  \n",
        "   Los llamados al LLM se realizan de forma consistente y reproducible,\n",
        "   asegurando que distintos usuarios obtengan resultados comparables.\n",
        "\n",
        "---\n",
        "\n",
        "### ¿Por qué no un modelo local?\n",
        "\n",
        "Modelos locales como Ollama,no alcanzan el rendimiento necesario para:\n",
        "\n",
        "- generar código correcto de Pandas o Cypher,  \n",
        "- comprender consultas ambiguas,  \n",
        "- reescribir respuestas con alta calidad.\n",
        "\n",
        "Además, los modelos locales suelen carecer de:\n",
        "- alineación fina para instrucciones,  \n",
        "- robustez ante consultas ruidosas,  \n",
        "- soporte multilingüe sólido.\n",
        "\n",
        "Por estas razones, se opta por un modelo en la nube."
      ],
      "metadata": {
        "id": "HxXFChcdOS0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llamar_llm(\n",
        "    prompt: str,\n",
        "    model: str = None,\n",
        "    retries: int = 3\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Llama al LLM de forma centralizada.\n",
        "\n",
        "    Parámetros:\n",
        "        prompt (str): Texto a enviar al LLM.\n",
        "        model (str): Nombre del modelo a usar.\n",
        "                     Si no se pasa, usa el modelo por defecto.\n",
        "        retries (int): Cantidad de reintentos en caso de error.\n",
        "\n",
        "    Devuelve:\n",
        "        str: Respuesta del modelo o mensaje de error.\n",
        "    \"\"\"\n",
        "\n",
        "    modelo = model if model is not None else modelo_gemini\n",
        "\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=modelo,\n",
        "                contents=[prompt]\n",
        "            )\n",
        "            return response.text.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error API (intento {i+1}/{retries}): {e}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    return \"⚠️ Error: El servicio de IA no está disponible en este momento.\"\n"
      ],
      "metadata": {
        "id": "z9vmgj5iOSUl"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justificación del Modelo de Lenguaje Utilizado (Gemini)\n",
        "\n",
        "El modelo elegido para este trabajo es **Gemini**, por las siguientes razones:\n",
        "\n",
        "1. **Excelente comprensión semántica en español:**  \n",
        "   El proyecto requiere interpretar consultas naturales sobre productos, ventas, compatibilidad, etc. Gemini muestra un desempeño superior en comprensión en español comparado con modelos\n",
        "   pequeños locales.\n",
        "\n",
        "2. **Capacidad para generar código:**  \n",
        "   En la fuente tabular y en la fuente grafo se necesita que el LLM genere:\n",
        "   - filtros de Pandas,\n",
        "   - consultas Cypher,\n",
        "   - expresiones condicionales,\n",
        "   - verificaciones básicas.\n",
        "\n",
        "   Gemini demuestra alta precisión en generación de código ejecutable.\n",
        "\n",
        "3. **Consistencia para clasificación de intención:**  \n",
        "   El clasificador LLM basado en few-shot funciona con una precisión perfecta en las pruebas, lo cual confirma la robustez del modelo para tareas de categorización semántica.\n",
        "\n",
        "4. **Velocidad y estabilidad:**  \n",
        "   La API de Gemini permite un tiempo de respuesta adecuado y no requiere gestión de tokens compleja por parte del usuario.\n",
        "\n",
        "5. **Integración con Python simplificada:**  \n",
        "   La librería oficial `google-generativeai` facilita la creación de clientes y la interacción directa con el modelo.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusión\n",
        "\n",
        "Gemini es una opción adecuada para este TP debido a:\n",
        "\n",
        "- su rendimiento,\n",
        "- su facilidad de integración,\n",
        "- su soporte multilingüe,\n",
        "- y su capacidad para generar código de calidad y respuestas contextuales.\n",
        "\n",
        "Por estas razones, se adopta como LLM principal del sistema.\n"
      ],
      "metadata": {
        "id": "YPfDDC3vOzpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interacción conversacional interactiva\n",
        "\n",
        "Para probar el sistema de manera más realista, se implementa un pequeño bucle\n",
        "interactivo en el que el usuario puede escribir preguntas manualmente y finalizar\n",
        "la sesión escribiendo `EXIT` o `SALIR`.\n",
        "\n",
        "En cada turno:\n",
        "\n",
        "1. El sistema clasifica la intención (vectorial / tabular / grafo).\n",
        "2. Ejecuta el pipeline correspondiente (con búsqueda híbrida y re-ranking en el caso vectorial).\n",
        "3. Genera una respuesta con el LLM.\n",
        "4. Registra la pregunta, la respuesta y la fuente utilizada en el historial de la conversación.\n",
        "\n",
        "Esto permite observar el comportamiento del asistente en un escenario similar al uso real\n",
        "y verificar que la memoria y la integración de componentes funcionan correctamente."
      ],
      "metadata": {
        "id": "QHrXF6uLQhrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crear_asistente_conversacional(metodo_clasificador: str = \"llm\"):\n",
        "    \"\"\"\n",
        "    Asistente conversacional oficial del TP.\n",
        "    Usa SIEMPRE el asistente avanzado (búsqueda híbrida + rerank).\n",
        "    Incluye historial para mantener memoria de la conversación.\n",
        "    \"\"\"\n",
        "\n",
        "    historial = []\n",
        "\n",
        "    def chat(pregunta: str):\n",
        "        nonlocal historial\n",
        "\n",
        "        resp = asistente_electro_avanzado(\n",
        "            pregunta,\n",
        "            historial=historial,\n",
        "            metodo_clasificador=metodo_clasificador,\n",
        "        )\n",
        "\n",
        "        # Guardamos memoria\n",
        "        historial.append({\n",
        "            \"pregunta\": pregunta,\n",
        "            \"respuesta\": resp[\"respuesta\"],\n",
        "            \"fuente\": resp[\"fuente\"],\n",
        "        })\n",
        "\n",
        "        return resp, historial\n",
        "\n",
        "    return chat\n"
      ],
      "metadata": {
        "id": "VO-ZiGVBOzWl"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear asistente conversacional\n",
        "chat = crear_asistente_conversacional(metodo_clasificador=\"ml\")\n",
        "\n",
        "print(\"Asistente Electro (versión final)\")\n",
        "print(\"Escribí tu pregunta en español. Escribí EXIT o SALIR para terminar.\\n\")\n",
        "\n",
        "while True:\n",
        "    pregunta = input(\"Usuario: \").strip()\n",
        "\n",
        "    if pregunta.lower() in [\"exit\", \"salir\", \"quit\"]:\n",
        "        print(\"\\nCerrando la conversación. Gracias por usar el asistente. 👋\")\n",
        "        break\n",
        "\n",
        "    resp, historial = chat(pregunta)\n",
        "\n",
        "    print(f\"\\nAsistente ({resp['fuente']}): {resp['respuesta']}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBCeKkBvQ28O",
        "outputId": "ca2e20a6-2b91-4330-a2fa-5070183cc09d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Asistente Electro (versión final)\n",
            "Escribí tu pregunta en español. Escribí EXIT o SALIR para terminar.\n",
            "\n",
            "Usuario: ¿Cómo uso mi licuadora para hacer smoothies?\n",
            "\n",
            "Asistente (vectorial): No tengo suficiente información para responder específicamente cómo usar tu licuadora para hacer smoothies. Las respuestas recuperadas mencionan que revises el manual del producto (código P0005 o P0006, dependiendo de si tu licuadora es HomeChef o ChefMaster) para más detalles sobre el uso correcto. Para obtener instrucciones precisas sobre cómo hacer smoothies, te sugiero que consultes el manual o contactes a nuestro servicio de atención al cliente.\n",
            "\n",
            "Usuario: ¿Cuáles son las licuadoras de menos de $400?\n",
            "🔧 Código generado por el modelo:\n",
            "\n",
            "productos[(productos['nombre'].str.contains('Licuadora', case=False)) & (productos['precio_usd'] < 400)]\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (tabular): Las siguientes licuadoras tienen un precio inferior a $400:\n",
            "\n",
            "*   Licuadora TechHome (Blanca, 283.63 USD)\n",
            "*   Plus Licuadora Pro TechHome (Negra, 329.07 USD)\n",
            "*   Compacto Licuadora ChefMaster (Rosa, 259.42 USD)\n",
            "\n",
            "Esto significa que estas tres licuadoras son las opciones disponibles que cumplen con tu criterio de precio.\n",
            "\n",
            "Usuario: ¿Qué opinan los usuarios de esta cafetera?\n",
            "🔧 Cypher generado por el modelo:\n",
            "\n",
            "MATCH (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto) WHERE p.nombre = \"Cafetera\" RETURN c.id AS id_producto, c.nombre AS nombre, c.categoria AS categoria, c.marca AS marca, r.comparte AS componente_compartido\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (grafo): No hay opiniones de usuarios disponibles para esa cafetera en este momento.\n",
            "\n",
            "Usuario: ¿Qué opinan los usuarios de la cafetera?\n",
            "\n",
            "Asistente (vectorial): Hay opiniones encontradas sobre la cafetera. Algunos usuarios están muy satisfechos: consideran que es eficiente, resistente, intuitiva, duradera, elegante y con una excelente relación calidad-precio (reseñas [1], [2] y [5]). Sin embargo, otros usuarios reportan mala calidad, lentitud y averías prematuras (reseña [3]). La FAQ [4] indica que el modelo \"Deluxe Cafetera 2024\" está diseñado para uso doméstico diario.\n",
            "\n",
            "Para darte una respuesta más completa, por favor, especificá a qué modelo de cafetera te referís o qué aspectos específicos te interesan conocer sobre la opinión de los usuarios (ej: durabilidad, facilidad de uso, etc.).\n",
            "\n",
            "Usuario: Quiero una licuadora con buenas reseñas\n",
            "🔧 Cypher generado por el modelo:\n",
            "\n",
            "MATCH (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto) WHERE p.nombre = \"licuadora\" RETURN c.id AS id_producto, c.nombre AS nombre, c.categoria AS categoria, c.marca AS marca, r.comparte AS componente_compartido\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (grafo): No se encontraron licuadoras con buenas reseñas en nuestra base de datos. Lo siento, no puedo ayudarte con tu solicitud.\n",
            "\n",
            "Usuario: Quiero una cafetera con buenas reseñas\n",
            "🔧 Cypher generado por el modelo:\n",
            "\n",
            "MATCH (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto) WHERE p.nombre = \"cafetera\" RETURN c.id AS id_producto, c.nombre AS nombre, c.categoria AS categoria, c.marca AS marca, r.comparte AS componente_compartido\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (grafo): No se encontraron cafeteras con buenas reseñas según la información disponible en la base de datos.\n",
            "\n",
            "Usuario: que productos son compatibles con la licuadora?\n",
            "🔧 Cypher generado por el modelo:\n",
            "\n",
            "MATCH (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto) WHERE p.nombre = \"Licuadora\" RETURN c.id AS id_producto, c.nombre AS nombre, c.categoria AS categoria, c.marca AS marca, r.comparte AS componente_compartido\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (grafo): No se encontraron productos compatibles con la licuadora en nuestra base de datos.\n",
            "\n",
            "Usuario: ¿Que licuadoras hay?\n",
            "🔧 Código generado por el modelo:\n",
            "\n",
            "productos[productos['nombre'].str.contains('licuadora', case=False)]\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (tabular): Hay 8 licuadoras disponibles. Se muestran sus identificadores, nombres, la marca, precio en dólares, el stock, color, potencia en watts, capacidad, voltaje, peso en kilogramos, los meses de garantía y una descripción.\n",
            "\n",
            "Usuario: ¿cuales son las licuadoras disponibles?\n",
            "🔧 Código generado por el modelo:\n",
            "\n",
            "productos[productos['nombre'].str.contains('Licuadora', case=False)]\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (tabular): Las licuadoras disponibles son:\n",
            "\n",
            "*   Licuadora TechHome (P0001)\n",
            "*   Licuadora TechHome (P0002)\n",
            "*   Plus Licuadora Pro TechHome (P0003)\n",
            "*   Compacto Licuadora ChefMaster (P0004)\n",
            "*   Licuadora HomeChef (P0005)\n",
            "*   Licuadora ChefMaster (P0006)\n",
            "*   Licuadora KitchenPro (P0007)\n",
            "*   Ultra Licuadora TechHome (P0008)\n",
            "\n",
            "La tabla muestra el detalle de cada licuadora, incluyendo su marca, precio, stock, color, potencia, capacidad y otras características relevantes.\n",
            "\n",
            "Usuario: ¿que productos son compatibles con la licuadora TechHome ?\n",
            "🔧 Cypher generado por el modelo:\n",
            "\n",
            "MATCH (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto) WHERE p.nombre = \"licuadora TechHome\" RETURN c.id AS id_producto, c.nombre AS nombre, c.categoria AS categoria, c.marca AS marca, r.comparte AS componente_compartido\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (grafo): No se encontraron productos compatibles con la licuadora TechHome.\n",
            "\n",
            "Usuario: ¿que productos son compatibles con la Licuadora ChefMaster?\n",
            "🔧 Cypher generado por el modelo:\n",
            "\n",
            "MATCH (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto) WHERE p.nombre = \"Licuadora ChefMaster\" RETURN c.id AS id_producto, c.nombre AS nombre, c.categoria AS categoria, c.marca AS marca, r.comparte AS componente_compartido\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (grafo): No se encontraron productos compatibles con la Licuadora ChefMaster en nuestra base de datos.\n",
            "\n",
            "Usuario: ¿Qué productos están relacionados con la categoría Cocina?”\n",
            "🔧 Cypher generado por el modelo:\n",
            "\n",
            "MATCH (p:Producto)-[r:COMPATIBLE_CON]->(c:Producto) WHERE p.categoria = \"Cocina\" RETURN c.id AS id_producto, c.nombre AS nombre, c.categoria AS categoria, c.marca AS marca, r.comparte AS componente_compartido\n",
            "\n",
            "---\n",
            "\n",
            "\n",
            "Asistente (grafo): Los productos relacionados con la categoría \"Cocina\" son:\n",
            "\n",
            "*   **Marca KitchenPro:** Advanced Batidora de Pie, Profesional Abridor de Latas, Turbo Exprimidor, Olla de Cocción Lenta, Turbo Microondas, Plus Parrilla Eléctrica, Turbo Abridor de Latas, Deluxe Yogurtera 3000, Olla de Cocción Lenta X, Smart Conservadora 2024, Procesadora, Deluxe Conservadora Pro, Max Pava Eléctrica, Abridor de Latas, Procesadora y Mixer. Algunos comparten componentes como \"Accesorios\", \"Jarra\", \"Panel de Control\", \"Cuchillas\" o \"Motor\".\n",
            "\n",
            "*   **Marca CookElite:** Eco Mixer II, Deluxe Abridor de Latas, Olla de Cocción Lenta II, Sandwichera, Pava Eléctrica, Profesional Yogurtera, Elite Heladera, Max Conservadora, Tostadora, Advanced Freezer, Vinoteca Pro, Smart Yogurtera, Parrilla Eléctrica, Freezer, Plus Olla de Cocción Lenta, Abridor de Latas 2024, Waflera, Digital Microondas, Elite Deshidratador, Premium Heladera, Digital Olla de Cocción Lenta. Algunos comparten componentes como \"Motor\", \"Jarra\", \"Accesorios\", \"Panel de Control\", \"Cuchillas\"\n",
            "\n",
            "*   **Marca HomeChef:** Frigobar, Eco Waflera, Digital Sandwichera Plus, Profesional Procesadora 2024, Abridor de Latas, Batidora de Pie II, Premium Molinillo de Café, Vinoteca II, Pava Eléctrica 2024, Turbo Conservadora, Premium Cafetera, Olla de Cocción Lenta Plus, Mixer II, Batidora de Mano II, Advanced Yogurtera, Sandwichera 3000, Pro Vinoteca, Pro Waflera, Digital Olla de Cocción Lenta, Mixer Pro, Algunos comparten componentes como \"Accesorios\", \"Jarra\", \"Panel de Control\", \"Cuchillas\" o \"Motor\".\n",
            "\n",
            "*   **Marca TechHome:** Compacto Horno Eléctrico, Vinoteca, Licuadora, Max Molinillo de Café, Freidora de Aire, Profesional Mixer, Olla de Cocción Lenta, Deluxe Procesadora, Ultra Parrilla Eléctrica, Profesional Freezer, Digital Rallador Eléctrico, Turbo Deshidratador, Yogurtera, Ultra Licuadora, Super Freezer, Elite Tostadora, Frigobar, Eco Rallador Eléctrico. Algunos comparten componentes como \"Panel de Control\", \"Jarra\", \"Cuchillas\", \"Accesorios\" o \"Motor\".\n",
            "\n",
            "*   **Marca ChefMaster:** Eco Conservadora II, Deluxe Exprimidor X, Advanced Heladera, Deshidratador, Waflera, Sandwichera 3000, Pro Vinoteca, Pro Waflera, Digital Olla de Cocción Lenta, Mixer Pro, Compacto Rallador Eléctrico 2024, Batidora de Mano, Licuadora. Algunos comparten componentes como \"Accesorios\", \"Panel de Control\", o \"Cuchillas\".\n",
            "\n",
            "Es importante destacar que los productos de diferentes marcas pueden compartir componentes entre sí.\n",
            "\n",
            "Usuario: salir\n",
            "\n",
            "Cerrando la conversación. Gracias por usar el asistente. 👋\n"
          ]
        }
      ]
    }
  ]
}